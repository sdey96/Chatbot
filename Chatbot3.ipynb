{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMfVFW1YRMbb0YsFLSnKgBm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Importing Necessary Libraries"],"metadata":{"id":"klFJeQkosrL1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3cC7hMftl1n"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","import string\n","import re\n","import random\n","from datetime import datetime\n","import glob"]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Bj5JCCNt2yV","executionInfo":{"status":"ok","timestamp":1722088113898,"user_tz":-330,"elapsed":1656,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}},"outputId":"ef5eb16f-c759-4edf-e8a6-bf467f57d1e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# Accesing the Files from Github to Read"],"metadata":{"id":"dF0rNN_Jsw3e"}},{"cell_type":"markdown","source":["### Load and Preprocess the Data"],"metadata":{"id":"dJAMkNgVtBgt"}},{"cell_type":"code","source":["!git clone https://github.com/sdey96/Chatbot.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"An6ClaJHZFuK","executionInfo":{"status":"ok","timestamp":1722095023623,"user_tz":-330,"elapsed":512,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}},"outputId":"b7dbf792-10b0-4a69-ea7e-0631bbb6cc7e"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'Chatbot' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["import os\n","\n","path = '/content/Chatbot'"],"metadata":{"id":"f9vggB55cA7k","executionInfo":{"status":"ok","timestamp":1722095027455,"user_tz":-330,"elapsed":2,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["file_paths = glob.glob(os.path.join(path, '*.txt'))"],"metadata":{"id":"Vf0cspCucMwE","executionInfo":{"status":"ok","timestamp":1722095030121,"user_tz":-330,"elapsed":666,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["raw_docs = []\n","for file_path in file_paths:\n","    with open(file_path, 'r', errors='ignore') as f:\n","        raw_docs.append(f.read().lower())"],"metadata":{"id":"2hc7OAjqc-lB","executionInfo":{"status":"ok","timestamp":1722095032971,"user_tz":-330,"elapsed":500,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":["### Combining into a Single String"],"metadata":{"id":"RbbTUR_PtK09"}},{"cell_type":"code","source":["combined_raw_doc = ' '.join(raw_docs)"],"metadata":{"id":"Ny17lB5O4Bfm","executionInfo":{"status":"ok","timestamp":1722095034649,"user_tz":-330,"elapsed":3,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["print(combined_raw_doc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Wy6uNVuhdJSH","executionInfo":{"status":"ok","timestamp":1722095037024,"user_tz":-330,"elapsed":6,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}},"outputId":"3002e092-bab9-4cf7-9207-701635cc219b"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. for classification tasks, the output of the random forest is the class selected by most trees. for regression tasks, the mean or average prediction of the individual trees is returned.[1][2] random decision forests correct for decision trees' habit of overfitting to their training set.[3]:‚Ää587‚Äì588‚Ää\n","\n","the first algorithm for random decision forests was created in 1995 by tin kam ho[1] using the random subspace method,[2] which, in ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by eugene kleinberg.[4][5][6]\n","\n","an extension of the algorithm was developed by leo breiman[7] and adele cutler,[8] who registered[9] \"random forests\" as a trademark in 2006 (as of 2019, owned by minitab, inc.).[10] the extension combines breiman's \"bagging\" idea and random selection of features, introduced first by ho[1] and later independently by amit and geman[11] in order to construct a collection of decision trees with controlled variance.\n","\n","history\n","the general method of random decision forests was first proposed by salzberg and heath in 1993,[12] with a method that used a randomized decision tree algorithm to generate multiple different trees and then combine them using majority voting. this idea was developed further by ho in 1995.[1] ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions. a subsequent work along the same lines[2] concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions. note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting. the explanation of the forest method's resistance to overtraining can be found in kleinberg's theory of stochastic discrimination.[4][5][6]\n","\n","the early development of breiman's notion of random forests was influenced by the work of amit and geman[11] who introduced the idea of searching over a random subset of the available decisions when splitting a node, in the context of growing a single tree. the idea of random subspace selection from ho[2] was also influential in the design of random forests. in this method a forest of trees is grown, and variation among the trees is introduced by projecting the training data into a randomly chosen subspace before fitting each tree or each node. finally, the idea of randomized node optimization, where the decision at each node is selected by a randomized procedure, rather than a deterministic optimization was first introduced by thomas g. dietterich.[13]\n","\n","the proper introduction of random forests was made in a paper by leo breiman.[7] this paper describes a method of building a forest of uncorrelated trees using a cart like procedure, combined with randomized node optimization and bagging. in addition, this paper combines several ingredients, some previously known and some novel, which form the basis of the modern practice of random forests, in particular:\n","\n","using out-of-bag error as an estimate of the generalization error.\n","measuring variable importance through permutation.\n","the report also offers the first theoretical result for random forests in the form of a bound on the generalization error which depends on the strength of the trees in the forest and their correlation.\n","\n","algorithm\n","preliminaries: decision tree learning\n","main article: decision tree learning\n","decision trees are a popular method for various machine learning tasks. tree learning \"come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining\", say hastie et al., \"because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. however, they are seldom accurate\".[3]:‚Ää352‚Ää\n","\n","in particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.[3]:‚Ää587‚Äì588‚Ää this comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.\n","\n","bagging\n","main article: bootstrap aggregating\n","\n","illustration of training a random forest model. the training dataset (in this case, of 250 rows and 100 columns) is randomly sampled with replacement n times. then, a decision tree is trained on each sample. finally, for prediction, the results of all n trees are aggregated to produce a final decision.\n","the training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. given a training set x = x1, ..., xn with responses y = y1, ..., yn, bagging repeatedly (b times) selects a random sample with replacement of the training set and fits trees to these samples:\n","\n","for b = 1, ..., b:\n","sample, with replacement, n training examples from x, y; call these xb, yb.\n","train a classification or regression tree fb on xb, yb.\n","after training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x':\n","\n","ùëì\n","^\n","=\n","1\n","ùêµ\n","‚àë\n","ùëè\n","=\n","1\n","ùêµ\n","ùëì\n","ùëè\n","(\n","ùë•\n","‚Ä≤\n",")\n","{\\displaystyle {\\hat {f}}={\\frac {1}{b}}\\sum _{b=1}^{b}f_{b}(x')}\n","\n","or by taking the plurality vote in the case of classification trees.\n","\n","this bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. this means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.\n","\n","additionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on x‚Ä≤:\n","ùúé\n","=\n","‚àë\n","ùëè\n","=\n","1\n","ùêµ\n","(\n","ùëì\n","ùëè\n","(\n","ùë•\n","‚Ä≤\n",")\n","‚àí\n","ùëì\n","^\n",")\n","2\n","ùêµ\n","‚àí\n","1\n",".\n","{\\displaystyle \\sigma ={\\sqrt {\\frac {\\sum _{b=1}^{b}(f_{b}(x')-{\\hat {f}})^{2}}{b-1}}}.}\n","\n","the number of samples/trees, b, is a free parameter. typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. an optimal number of trees b can be found using cross-validation, or by observing the out-of-bag error: the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample.[14] the training and test error tend to level off after some number of trees have been fit.\n","\n","from bagging to random forests\n","main article: random subspace method\n","the above procedure describes the original bagging algorithm for trees. random forests also include another type of bagging scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. this process is sometimes called \"feature bagging\". the reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the b trees, causing them to become correlated. an analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by ho.[15]\n","\n","typically, for a classification problem with p features, ‚àöp (rounded down) features are used in each split.[3]:‚Ää592‚Ää for regression problems the inventors recommend p/3 (rounded down) with a minimum node size of 5 as the default.[3]:‚Ää592‚Ää in practice, the best values for these parameters should be tuned on a case-to-case basis for every problem.[3]:‚Ää592‚Ää\n","\n","extratrees\n","adding one further step of randomization yields extremely randomized trees, or extratrees. while similar to ordinary random forests in that they are an ensemble of individual trees, there are two main differences: first, each tree is trained using the whole learning sample (rather than a bootstrap sample), and second, the top-down splitting in the tree learner is randomized. instead of computing the locally optimal cut-point for each feature under consideration (based on, e.g., information gain or the gini impurity), a random cut-point is selected. this value is selected from a uniform distribution within the feature's empirical range (in the tree's training set). then, of all the randomly generated splits, the split that yields the highest score is chosen to split the node. similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. default values for this parameter are \n","ùëù\n","{\\displaystyle {\\sqrt {p}}} for classification and \n","ùëù\n","{\\displaystyle p} for regression, where \n","ùëù\n","{\\displaystyle p} is the number of features in the model.[16]\n","\n","random forests for high-dimensional data\n","the basic random forest procedure may not work well in situations where there are a large number of features but only a small proportion of these features are informative with respect to sample classification. this can be addressed by encouraging the procedure to focus mainly on features and trees that are informative. some methods for accomplishing this are:\n","\n","prefiltering: eliminate features that are mostly just noise.[17][18]\n","enriched random forest (erf): use weighted random sampling instead of simple random sampling at each node of each tree, giving greater weight to features that appear to be more informative.[19][20]\n","tree weighted random forest (twrf): weight trees so that trees exhibiting better accuracy are assigned higher weights.[21][22]\n","properties\n","variable importance\n","random forests can be used to rank the importance of variables in a regression or classification problem in a natural way. the following technique was described in breiman's original paper[7] and is implemented in the r package randomforest.[8]\n","\n","permutation importance\n","the first step in measuring the variable importance in a data set \n","ùê∑\n","ùëõ\n","=\n","{\n","(\n","ùëã\n","ùëñ\n",",\n","ùëå\n","ùëñ\n",")\n","}\n","ùëñ\n","=\n","1\n","ùëõ\n","{\\displaystyle {\\mathcal {d}}_{n}=\\{(x_{i},y_{i})\\}_{i=1}^{n}} is to fit a random forest to the data. during the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).\n","\n","to measure the importance of the \n","ùëó\n","{\\displaystyle j}-th feature after training, the values of the \n","ùëó\n","{\\displaystyle j}-th feature are permuted in the out-of-bag samples and the out-of-bag error is again computed on this perturbed data set. the importance score for the \n","ùëó\n","{\\displaystyle j}-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. the score is normalized by the standard deviation of these differences.\n","\n","features which produce large values for this score are ranked as more important than features which produce small values. the statistical definition of the variable importance measure was given and analyzed by zhu et al.[23]\n","\n","this method of determining variable importance has some drawbacks.\n","\n","for data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. methods such as partial permutations[24][25][26] and growing unbiased trees[27][28] can be used to solve the problem.\n","if the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.[29]\n","additionally, the permutation procedure may fail to identify important features when there are collinear features. in this case permuting groups of correlated features together is a remedy.[30]\n","mean decrease in impurity feature importance\n","this feature importance for random forests is the default implementation in sci-kit learn and r. it is described in the book \"classification and regression trees\" by leo breiman.[31] variables which decrease the impurity during splits a lot are considered important:[32]\n","unormalized average importance\n","(\n","ùë•\n",")\n","=\n","1\n","ùëõ\n","ùëá\n","‚àë\n","ùëñ\n","=\n","1\n","ùëõ\n","ùëá\n","‚àë\n","node \n","ùëó\n","‚àà\n","ùëá\n","ùëñ\n","|\n","split variable\n","(\n","ùëó\n",")\n","=\n","ùë•\n","ùëù\n","ùëá\n","ùëñ\n","(\n","ùëó\n",")\n","Œ¥\n","ùëñ\n","ùëá\n","ùëñ\n","(\n","ùëó\n",")\n",",\n","{\\displaystyle {\\text{unormalized average importance}}(x)={\\frac {1}{n_{t}}}\\sum _{i=1}^{n_{t}}\\sum _{{\\text{node }}j\\in t_{i}|{\\text{split variable}}(j)=x}p_{t_{i}}(j)\\delta i_{t_{i}}(j),}where \n","ùë•\n","{\\displaystyle x} indicates a feature, \n","ùëõ\n","ùëá\n","{\\displaystyle n_{t}} is the number of trees in the forest, \n","ùëá\n","ùëñ\n","{\\displaystyle t_{i}} indicates tree \n","ùëñ\n","{\\displaystyle i}, \n","ùëù\n","ùëá\n","ùëñ\n","(\n","ùëó\n",")\n","=\n","ùëõ\n","ùëó\n","ùëõ\n","{\\displaystyle p_{t_{i}}(j)={\\frac {n_{j}}{n}}} is the fraction of samples reaching node \n","ùëó\n","{\\displaystyle j}, \n","Œ¥\n","ùëñ\n","ùëá\n","ùëñ\n","(\n","ùëó\n",")\n","{\\displaystyle \\delta i_{t_{i}}(j)} is the change in impurity in tree \n","ùë°\n","{\\displaystyle t} at node \n","ùëó\n","{\\displaystyle j}. as impurity measure for samples falling in a node e.g. the following statistics can be used:\n","\n","entropy\n","gini coefficient\n","mean squared error\n","the normalized importance is then obtained by normalizing over all features, so that the sum of normalized feature importances is 1.\n","\n","the sci-kit learn default implementation of mean decrease in impurity feature importance is susceptible to misleading feature importances:[30]\n","\n","the importance measure prefers high cardinality features\n","it uses training statistics and therefore does not \"reflect the ability of feature to be useful to make predictions that generalize to the test set\"[33]\n","relationship to nearest neighbors\n","a relationship between random forests and the k-nearest neighbor algorithm (k-nn) was pointed out by lin and jeon in 2002.[34] it turns out that both can be viewed as so-called weighted neighborhoods schemes. these are models built from a training set \n","{\n","(\n","ùë•\n","ùëñ\n",",\n","ùë¶\n","ùëñ\n",")\n","}\n","ùëñ\n","=\n","1\n","ùëõ\n","{\\displaystyle \\{(x_{i},y_{i})\\}_{i=1}^{n}} that make predictions \n","ùë¶\n","^{\\displaystyle {\\hat {y}}} for new points x' by looking at the \"neighborhood\" of the point, formalized by a weight function w:\n","ùë¶\n","^\n","=\n","‚àë\n","ùëñ\n","=\n","1\n","ùëõ\n","ùëä\n","(\n","ùë•\n","ùëñ\n",",\n","ùë•\n","‚Ä≤\n",")\n","ùë¶\n","ùëñ\n",".\n","{\\displaystyle {\\hat {y}}=\\sum _{i=1}^{n}w(x_{i},x')\\,y_{i}.}\n","\n","here, \n","ùëä\n","(\n","ùë•\n","ùëñ\n",",\n","ùë•\n","‚Ä≤\n",")\n","{\\displaystyle w(x_{i},x')} is the non-negative weight of the i'th training point relative to the new point x' in the same tree. for any particular x', the weights for points \n","ùë•\n","ùëñ\n","{\\displaystyle x_{i}} must sum to one. weight functions are given as follows:\n","\n","in k-nn, the weights are \n","ùëä\n","(\n","ùë•\n","ùëñ\n",",\n","ùë•\n","‚Ä≤\n",")\n","=\n","1\n","ùëò\n","{\\displaystyle w(x_{i},x')={\\frac {1}{k}}} if xi is one of the k points closest to x', and zero otherwise.\n","in a tree, \n","ùëä\n","(\n","ùë•\n","ùëñ\n",",\n","ùë•\n","‚Ä≤\n",")\n","=\n","1\n","ùëò\n","‚Ä≤\n","{\\displaystyle w(x_{i},x')={\\frac {1}{k'}}} if xi is one of the k' points in the same leaf as x', and zero otherwise.\n","since a forest averages the predictions of a set of m trees with individual weight functions \n","ùëä\n","ùëó\n","{\\displaystyle w_{j}}, its predictions are\n","ùë¶\n","^\n","=\n","1\n","ùëö\n","‚àë\n","ùëó\n","=\n","1\n","ùëö\n","‚àë\n","ùëñ\n","=\n","1\n","ùëõ\n","ùëä\n","ùëó\n","(\n","ùë•\n","ùëñ\n",",\n","ùë•\n","‚Ä≤\n",")\n","ùë¶\n","ùëñ\n","=\n","‚àë\n","ùëñ\n","=\n","1\n","ùëõ\n","(\n","1\n","ùëö\n","‚àë\n","ùëó\n","=\n","1\n","ùëö\n","ùëä\n","ùëó\n","(\n","ùë•\n","ùëñ\n",",\n","ùë•\n","‚Ä≤\n",")\n",")\n","ùë¶\n","ùëñ\n",".\n","{\\displaystyle {\\hat {y}}={\\frac {1}{m}}\\sum _{j=1}^{m}\\sum _{i=1}^{n}w_{j}(x_{i},x')\\,y_{i}=\\sum _{i=1}^{n}\\left({\\frac {1}{m}}\\sum _{j=1}^{m}w_{j}(x_{i},x')\\right)\\,y_{i}.}\n","\n","this shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. the neighbors of x' in this interpretation are the points \n","ùë•\n","ùëñ\n","{\\displaystyle x_{i}} sharing the same leaf in any tree \n","ùëó\n","{\\displaystyle j}. in this way, the neighborhood of x' depends in a complex way on the structure of the trees, and thus on the structure of the training set. lin and jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.[34]\n","\n","unsupervised learning with random forests\n","as part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. one can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the \"observed\" data from suitably generated synthetic data.[7][35] the observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. a random forest dissimilarity can be attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. the random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the \"addcl 1\" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. the random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.[36]\n","\n","variants\n","instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive bayes classifiers.[37][38][39] in cases that the relationship between the predictors and the target variable is linear, the base learners may have an equally high accuracy as the ensemble learner.[40][37]\n","\n","kernel random forest\n","in machine learning, kernel random forests (kerf) establish the connection between random forests and kernel methods. by slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.[41]\n","\n","history\n","leo breiman[42] was the first person to notice the link between random forest and kernel methods. he pointed out that random forests which are grown using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. lin and jeon[43] established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. davies and ghahramani[44] proposed random forest kernel and show that it can empirically outperform state-of-art kernel methods. scornet[41] first defined kerf estimates and gave the explicit link between kerf estimates and random forest. he also gave explicit expressions for kernels based on centered random forest[45] and uniform random forest,[46] two simplified models of random forest. he named these two kerfs centered kerf and uniform kerf, and proved upper bounds on their rates of consistency.\n","\n","notations and definitions\n","preliminaries: centered forests\n","centered forest[45] is a simplified model for breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. the algorithm stops when a fully binary tree of level \n","ùëò\n","{\\displaystyle k} is built, where \n","ùëò\n","‚àà\n","ùëÅ\n","{\\displaystyle k\\in \\mathbb {n} } is a parameter of the algorithm.\n","\n","uniform forest\n","uniform forest[46] is another simplified model for breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.\n","\n","from random forest to kerf\n","given a training sample \n","ùê∑\n","ùëõ\n","=\n","{\n","(\n","ùëã\n","ùëñ\n",",\n","ùëå\n","ùëñ\n",")\n","}\n","ùëñ\n","=\n","1\n","ùëõ\n","{\\displaystyle {\\mathcal {d}}_{n}=\\{(\\mathbf {x} _{i},y_{i})\\}_{i=1}^{n}} of \n","[\n","0\n",",\n","1\n","]\n","ùëù\n","√ó\n","ùëÖ\n","{\\displaystyle [0,1]^{p}\\times \\mathbb {r} }-valued independent random variables distributed as the independent prototype pair \n","(\n","ùëã\n",",\n","ùëå\n",")\n","{\\displaystyle (\\mathbf {x} ,y)}, where \n","e\n","‚Å°\n","[\n","ùëå\n","2\n","]\n","<\n","‚àû{\\displaystyle \\operatorname {e} [y^{2}]<\\infty }. we aim at predicting the response \n","ùëå\n","{\\displaystyle y}, associated with the random variable \n","ùëã\n","{\\displaystyle \\mathbf {x} }, by estimating the regression function \n","ùëö\n","(\n","ùë•\n",")\n","=\n","e\n","‚Å°\n","[\n","ùëå\n","‚à£\n","ùëã\n","=\n","ùë•\n","]\n","{\\displaystyle m(\\mathbf {x} )=\\operatorname {e} [y\\mid \\mathbf {x} =\\mathbf {x} ]}. a random regression forest is an ensemble of \n","ùëÄ\n","{\\displaystyle m} randomized regression trees. denote \n","ùëö\n","ùëõ\n","(\n","ùë•\n",",\n","ùõ©\n","ùëó\n",")\n","{\\displaystyle m_{n}(\\mathbf {x} ,\\mathbf {\\theta } _{j})} the predicted value at point \n","ùë•\n","{\\displaystyle \\mathbf {x} } by the \n","ùëó\n","{\\displaystyle j}-th tree, where \n","ùõ©\n","1\n",",\n","‚Ä¶\n",",\n","ùõ©\n","ùëÄ\n","{\\displaystyle \\mathbf {\\theta } _{1},\\ldots ,\\mathbf {\\theta } _{m}} are independent random variables, distributed as a generic random variable \n","ùõ©{\\displaystyle \\mathbf {\\theta } }, independent of the sample \n","ùê∑\n","ùëõ\n","{\\displaystyle {\\mathcal {d}}_{n}}. this random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. the trees are combined to form the finite forest estimate \n","ùëö\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","1\n",",\n","‚Ä¶\n",",\n","Œ∏\n","ùëÄ\n",")\n","=\n","1\n","ùëÄ\n","‚àë\n","ùëó\n","=\n","1\n","ùëÄ\n","ùëö\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n","{\\displaystyle m_{m,n}(\\mathbf {x} ,\\theta _{1},\\ldots ,\\theta _{m})={\\frac {1}{m}}\\sum _{j=1}^{m}m_{n}(\\mathbf {x} ,\\theta _{j})}. for regression trees, we have \n","ùëö\n","ùëõ\n","=\n","‚àë\n","ùëñ\n","=\n","1\n","ùëõ\n","ùëå\n","ùëñ\n","1\n","ùëã\n","ùëñ\n","‚àà\n","ùê¥\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n","ùëÅ\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n","{\\displaystyle m_{n}=\\sum _{i=1}^{n}{\\frac {y_{i}\\mathbf {1} _{\\mathbf {x} _{i}\\in a_{n}(\\mathbf {x} ,\\theta _{j})}}{n_{n}(\\mathbf {x} ,\\theta _{j})}}}, where \n","ùê¥\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n","{\\displaystyle a_{n}(\\mathbf {x} ,\\theta _{j})} is the cell containing \n","ùë•\n","{\\displaystyle \\mathbf {x} }, designed with randomness \n","Œ∏\n","ùëó\n","{\\displaystyle \\theta _{j}} and dataset \n","ùê∑\n","ùëõ\n","{\\displaystyle {\\mathcal {d}}_{n}}, and \n","ùëÅ\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n","=\n","‚àë\n","ùëñ\n","=\n","1\n","ùëõ\n","1\n","ùëã\n","ùëñ\n","‚àà\n","ùê¥\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n","{\\displaystyle n_{n}(\\mathbf {x} ,\\theta _{j})=\\sum _{i=1}^{n}\\mathbf {1} _{\\mathbf {x} _{i}\\in a_{n}(\\mathbf {x} ,\\theta _{j})}}.\n","\n","thus random forest estimates satisfy, for all \n","ùë•\n","‚àà\n","[\n","0\n",",\n","1\n","]\n","ùëë\n","{\\displaystyle \\mathbf {x} \\in [0,1]^{d}}, \n","ùëö\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","1\n",",\n","‚Ä¶\n",",\n","Œ∏\n","ùëÄ\n",")\n","=\n","1\n","ùëÄ\n","‚àë\n","ùëó\n","=\n","1\n","ùëÄ\n","(\n","‚àë\n","ùëñ\n","=\n","1\n","ùëõ\n","ùëå\n","ùëñ\n","1\n","ùëã\n","ùëñ\n","‚àà\n","ùê¥\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n","ùëÅ\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n",")\n","{\\displaystyle m_{m,n}(\\mathbf {x} ,\\theta _{1},\\ldots ,\\theta _{m})={\\frac {1}{m}}\\sum _{j=1}^{m}\\left(\\sum _{i=1}^{n}{\\frac {y_{i}\\mathbf {1} _{\\mathbf {x} _{i}\\in a_{n}(\\mathbf {x} ,\\theta _{j})}}{n_{n}(\\mathbf {x} ,\\theta _{j})}}\\right)}. random regression forest has two levels of averaging, first over the samples in the target cell of a tree, then over all trees. thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. in order to improve the random forest methods and compensate the misestimation, scornet[41] defined kerf by\n","ùëö\n","~\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","1\n",",\n","‚Ä¶\n",",\n","Œ∏\n","ùëÄ\n",")\n","=\n","1\n","‚àë\n","ùëó\n","=\n","1\n","ùëÄ\n","ùëÅ\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n","‚àë\n","ùëó\n","=\n","1\n","ùëÄ\n","‚àë\n","ùëñ\n","=\n","1\n","ùëõ\n","ùëå\n","ùëñ\n","1\n","ùëã\n","ùëñ\n","‚àà\n","ùê¥\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n",",\n","{\\displaystyle {\\tilde {m}}_{m,n}(\\mathbf {x} ,\\theta _{1},\\ldots ,\\theta _{m})={\\frac {1}{\\sum _{j=1}^{m}n_{n}(\\mathbf {x} ,\\theta _{j})}}\\sum _{j=1}^{m}\\sum _{i=1}^{n}y_{i}\\mathbf {1} _{\\mathbf {x} _{i}\\in a_{n}(\\mathbf {x} ,\\theta _{j})},}which is equal to the mean of the \n","ùëå\n","ùëñ\n","{\\displaystyle y_{i}}'s falling in the cells containing \n","ùë•\n","{\\displaystyle \\mathbf {x} } in the forest. if we define the connection function of the \n","ùëÄ\n","{\\displaystyle m} finite forest as \n","ùêæ\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",",\n","ùëß\n",")\n","=\n","1\n","ùëÄ\n","‚àë\n","ùëó\n","=\n","1\n","ùëÄ\n","1\n","ùëß\n","‚àà\n","ùê¥\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","ùëó\n",")\n","{\\displaystyle k_{m,n}(\\mathbf {x} ,\\mathbf {z} )={\\frac {1}{m}}\\sum _{j=1}^{m}\\mathbf {1} _{\\mathbf {z} \\in a_{n}(\\mathbf {x} ,\\theta _{j})}}, i.e. the proportion of cells shared between \n","ùë•\n","{\\displaystyle \\mathbf {x} } and \n","ùëß\n","{\\displaystyle \\mathbf {z} }, then almost surely we have \n","ùëö\n","~\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","1\n",",\n","‚Ä¶\n",",\n","Œ∏\n","ùëÄ\n",")\n","=\n","‚àë\n","ùëñ\n","=\n","1\n","ùëõ\n","ùëå\n","ùëñ\n","ùêæ\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",",\n","ùë•\n","ùëñ\n",")\n","‚àë\n","‚Ñì\n","=\n","1\n","ùëõ\n","ùêæ\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",",\n","ùë•\n","‚Ñì\n",")\n","{\\displaystyle {\\tilde {m}}_{m,n}(\\mathbf {x} ,\\theta _{1},\\ldots ,\\theta _{m})={\\frac {\\sum _{i=1}^{n}y_{i}k_{m,n}(\\mathbf {x} ,\\mathbf {x} _{i})}{\\sum _{\\ell =1}^{n}k_{m,n}(\\mathbf {x} ,\\mathbf {x} _{\\ell })}}}, which defines the kerf.\n","\n","centered kerf\n","the construction of centered kerf of level \n","ùëò\n","{\\displaystyle k} is the same as for centered forest, except that predictions are made by \n","ùëö\n","~\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","1\n",",\n","‚Ä¶\n",",\n","Œ∏\n","ùëÄ\n",")\n","{\\displaystyle {\\tilde {m}}_{m,n}(\\mathbf {x} ,\\theta _{1},\\ldots ,\\theta _{m})}, the corresponding kernel function, or connection function is\n","ùêæ\n","ùëò\n","ùëê\n","ùëê\n","(\n","ùë•\n",",\n","ùëß\n",")\n","=\n","‚àë\n","ùëò\n","1\n",",\n","‚Ä¶\n",",\n","ùëò\n","ùëë\n",",\n","‚àë\n","ùëó\n","=\n","1\n","ùëë\n","ùëò\n","ùëó\n","=\n","ùëò\n","ùëò\n","!\n","ùëò\n","1\n","!\n","‚ãØ\n","ùëò\n","ùëë\n","!\n","(\n","1\n","ùëë\n",")\n","ùëò\n","‚àè\n","ùëó\n","=\n","1\n","ùëë\n","1\n","‚åà\n","2\n","ùëò\n","ùëó\n","ùë•\n","ùëó\n","‚åâ\n","=\n","‚åà\n","2\n","ùëò\n","ùëó\n","ùëß\n","ùëó\n","‚åâ\n",",\n"," for all \n","ùë•\n",",\n","ùëß\n","‚àà\n","[\n","0\n",",\n","1\n","]\n","ùëë\n",".\n","{\\displaystyle k_{k}^{cc}(\\mathbf {x} ,\\mathbf {z} )=\\sum _{k_{1},\\ldots ,k_{d},\\sum _{j=1}^{d}k_{j}=k}{\\frac {k!}{k_{1}!\\cdots k_{d}!}}\\left({\\frac {1}{d}}\\right)^{k}\\prod _{j=1}^{d}\\mathbf {1} _{\\lceil 2^{k_{j}}x_{j}\\rceil =\\lceil 2^{k_{j}}z_{j}\\rceil },\\qquad {\\text{ for all }}\\mathbf {x} ,\\mathbf {z} \\in [0,1]^{d}.}\n","\n","uniform kerf\n","uniform kerf is built in the same way as uniform forest, except that predictions are made by \n","ùëö\n","~\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n","1\n",",\n","‚Ä¶\n",",\n","Œ∏\n","ùëÄ\n",")\n","{\\displaystyle {\\tilde {m}}_{m,n}(\\mathbf {x} ,\\theta _{1},\\ldots ,\\theta _{m})}, the corresponding kernel function, or connection function is\n","ùêæ\n","ùëò\n","ùë¢\n","ùëì\n","(\n","0\n",",\n","ùë•\n",")\n","=\n","‚àë\n","ùëò\n","1\n",",\n","‚Ä¶\n",",\n","ùëò\n","ùëë\n",",\n","‚àë\n","ùëó\n","=\n","1\n","ùëë\n","ùëò\n","ùëó\n","=\n","ùëò\n","ùëò\n","!\n","ùëò\n","1\n","!\n","‚Ä¶\n","ùëò\n","ùëë\n","!\n","(\n","1\n","ùëë\n",")\n","ùëò\n","‚àè\n","ùëö\n","=\n","1\n","ùëë\n","(\n","1\n","‚àí\n","|\n","ùë•\n","ùëö\n","|\n","‚àë\n","ùëó\n","=\n","0\n","ùëò\n","ùëö\n","‚àí\n","1\n","(\n","‚àí\n","ln\n","‚Å°\n","|\n","ùë•\n","ùëö\n","|\n",")\n","ùëó\n","ùëó\n","!\n",")\n"," for all \n","ùë•\n","‚àà\n","[\n","0\n",",\n","1\n","]\n","ùëë\n",".\n","{\\displaystyle k_{k}^{uf}(\\mathbf {0} ,\\mathbf {x} )=\\sum _{k_{1},\\ldots ,k_{d},\\sum _{j=1}^{d}k_{j}=k}{\\frac {k!}{k_{1}!\\ldots k_{d}!}}\\left({\\frac {1}{d}}\\right)^{k}\\prod _{m=1}^{d}\\left(1-|x_{m}|\\sum _{j=0}^{k_{m}-1}{\\frac {\\left(-\\ln |x_{m}|\\right)^{j}}{j!}}\\right){\\text{ for all }}\\mathbf {x} \\in [0,1]^{d}.}\n","\n","properties\n","relation between kerf and random forest\n","predictions given by kerf and random forests are close if the number of points in each cell is controlled:\n","\n","assume that there exist sequences \n","(\n","ùëé\n","ùëõ\n",")\n",",\n","(\n","ùëè\n","ùëõ\n",")\n","{\\displaystyle (a_{n}),(b_{n})} such that, almost surely,\n","ùëé\n","ùëõ\n","‚â§\n","ùëÅ\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n",")\n","‚â§\n","ùëè\n","ùëõ\n"," and \n","ùëé\n","ùëõ\n","‚â§\n","1\n","ùëÄ\n","‚àë\n","ùëö\n","=\n","1\n","ùëÄ\n","ùëÅ\n","ùëõ\n","ùë•\n",",\n","Œ∏\n","ùëö\n","‚â§\n","ùëè\n","ùëõ\n",".\n","{\\displaystyle a_{n}\\leq n_{n}(\\mathbf {x} ,\\theta )\\leq b_{n}{\\text{ and }}a_{n}\\leq {\\frac {1}{m}}\\sum _{m=1}^{m}n_{n}{\\mathbf {x} ,\\theta _{m}}\\leq b_{n}.}then almost surely,\n","|\n","ùëö\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",")\n","‚àí\n","ùëö\n","~\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",")\n","|\n","‚â§\n","ùëè\n","ùëõ\n","‚àí\n","ùëé\n","ùëõ\n","ùëé\n","ùëõ\n","ùëö\n","~\n","ùëÄ\n",",\n","ùëõ\n","(\n","ùë•\n",")\n",".\n","{\\displaystyle |m_{m,n}(\\mathbf {x} )-{\\tilde {m}}_{m,n}(\\mathbf {x} )|\\leq {\\frac {b_{n}-a_{n}}{a_{n}}}{\\tilde {m}}_{m,n}(\\mathbf {x} ).}\n","\n","relation between infinite kerf and infinite random forest\n","when the number of trees \n","ùëÄ\n","{\\displaystyle m} goes to infinity, then we have infinite random forest and infinite kerf. their estimates are close if the number of observations in each cell is bounded:\n","\n","assume that there exist sequences \n","(\n","ùúÄ\n","ùëõ\n",")\n",",\n","(\n","ùëé\n","ùëõ\n",")\n",",\n","(\n","ùëè\n","ùëõ\n",")\n","{\\displaystyle (\\varepsilon _{n}),(a_{n}),(b_{n})} such that, almost surely\n","\n","e\n","‚Å°\n","[\n","ùëÅ\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n",")\n","]\n","‚â•\n","1\n",",\n","{\\displaystyle \\operatorname {e} [n_{n}(\\mathbf {x} ,\\theta )]\\geq 1,}\n","p\n","‚Å°\n","[\n","ùëé\n","ùëõ\n","‚â§\n","ùëÅ\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n",")\n","‚â§\n","ùëè\n","ùëõ\n","‚à£\n","ùê∑\n","ùëõ\n","]\n","‚â•\n","1\n","‚àí\n","ùúÄ\n","ùëõ\n","/\n","2\n",",\n","{\\displaystyle \\operatorname {p} [a_{n}\\leq n_{n}(\\mathbf {x} ,\\theta )\\leq b_{n}\\mid {\\mathcal {d}}_{n}]\\geq 1-\\varepsilon _{n}/2,}\n","p\n","‚Å°\n","[\n","ùëé\n","ùëõ\n","‚â§\n","e\n","Œ∏\n","‚Å°\n","[\n","ùëÅ\n","ùëõ\n","(\n","ùë•\n",",\n","Œ∏\n",")\n","]\n","‚â§\n","ùëè\n","ùëõ\n","‚à£\n","ùê∑\n","ùëõ\n","]\n","‚â•\n","1\n","‚àí\n","ùúÄ\n","ùëõ\n","/\n","2\n",",\n","{\\displaystyle \\operatorname {p} [a_{n}\\leq \\operatorname {e} _{\\theta }[n_{n}(\\mathbf {x} ,\\theta )]\\leq b_{n}\\mid {\\mathcal {d}}_{n}]\\geq 1-\\varepsilon _{n}/2,}\n","then almost surely,\n","|\n","ùëö\n","‚àû\n",",\n","ùëõ\n","(\n","ùë•\n",")\n","‚àí\n","ùëö\n","~\n","‚àû\n",",\n","ùëõ\n","(\n","ùë•\n",")\n","|\n","‚â§\n","ùëè\n","ùëõ\n","‚àí\n","ùëé\n","ùëõ\n","ùëé\n","ùëõ\n","ùëö\n","~\n","‚àû\n",",\n","ùëõ\n","(\n","ùë•\n",")\n","+\n","ùëõ\n","ùúÄ\n","ùëõ\n","(\n","max\n","1\n","‚â§\n","ùëñ\n","‚â§\n","ùëõ\n","ùëå\n","ùëñ\n",")\n",".\n","{\\displaystyle |m_{\\infty ,n}(\\mathbf {x} )-{\\tilde {m}}_{\\infty ,n}(\\mathbf {x} )|\\leq {\\frac {b_{n}-a_{n}}{a_{n}}}{\\tilde {m}}_{\\infty ,n}(\\mathbf {x} )+n\\varepsilon _{n}\\left(\\max _{1\\leq i\\leq n}y_{i}\\right).}\n","\n","consistency results\n","assume that \n","ùëå\n","=\n","ùëö\n","(\n","ùëã\n",")\n","+\n","ùúÄ{\\displaystyle y=m(\\mathbf {x} )+\\varepsilon }, where \n","ùúÄ{\\displaystyle \\varepsilon } is a centered gaussian noise, independent of \n","ùëã\n","{\\displaystyle \\mathbf {x} }, with finite variance \n","ùúé\n","2\n","<\n","‚àû{\\displaystyle \\sigma ^{2}<\\infty }. moreover, \n","ùëã\n","{\\displaystyle \\mathbf {x} } is uniformly distributed on \n","[\n","0\n",",\n","1\n","]\n","ùëë\n","{\\displaystyle [0,1]^{d}} and \n","ùëö\n","{\\displaystyle m} is lipschitz. scornet[41] proved upper bounds on the rates of consistency for centered kerf and uniform kerf.\n","\n","consistency of centered kerf\n","providing \n","ùëò\n","‚Üí\n","‚àû{\\displaystyle k\\rightarrow \\infty } and \n","ùëõ\n","/\n","2\n","ùëò\n","‚Üí\n","‚àû{\\displaystyle n/2^{k}\\rightarrow \\infty }, there exists a constant \n","ùê∂\n","1\n",">\n","0\n","{\\displaystyle c_{1}>0} such that, for all \n","ùëõ\n","{\\displaystyle n}, \n","ùê∏\n","[\n","ùëö\n","~\n","ùëõ\n","ùëê\n","ùëê\n","(\n","ùëã\n",")\n","‚àí\n","ùëö\n","(\n","ùëã\n",")\n","]\n","2\n","‚â§\n","ùê∂\n","1\n","ùëõ\n","‚àí\n","1\n","/\n","(\n","3\n","+\n","ùëë\n","log\n","‚Å°\n","2\n",")\n","(\n","log\n","‚Å°\n","ùëõ\n",")\n","2\n","{\\displaystyle \\mathbb {e} [{\\tilde {m}}_{n}^{cc}(\\mathbf {x} )-m(\\mathbf {x} )]^{2}\\leq c_{1}n^{-1/(3+d\\log 2)}(\\log n)^{2}}.\n","\n","consistency of uniform kerf\n","providing \n","ùëò\n","‚Üí\n","‚àû{\\displaystyle k\\rightarrow \\infty } and \n","ùëõ\n","/\n","2\n","ùëò\n","‚Üí\n","‚àû{\\displaystyle n/2^{k}\\rightarrow \\infty }, there exists a constant \n","ùê∂\n",">\n","0\n","{\\displaystyle c>0} such that, \n","ùê∏\n","[\n","ùëö\n","~\n","ùëõ\n","ùë¢\n","ùëì\n","(\n","ùëã\n",")\n","‚àí\n","ùëö\n","(\n","ùëã\n",")\n","]\n","2\n","‚â§\n","ùê∂\n","ùëõ\n","‚àí\n","2\n","/\n","(\n","6\n","+\n","3\n","ùëë\n","log\n","‚Å°\n","2\n",")\n","(\n","log\n","‚Å°\n","ùëõ\n",")\n","2\n","{\\displaystyle \\mathbb {e} [{\\tilde {m}}_{n}^{uf}(\\mathbf {x} )-m(\\mathbf {x} )]^{2}\\leq cn^{-2/(6+3d\\log 2)}(\\log n)^{2}}.\n","\n","disadvantages\n","while random forests often achieve higher accuracy than a single decision tree, they sacrifice the intrinsic interpretability present in decision trees. decision trees are among a fairly small family of machine learning models that are easily interpretable along with linear models, rule-based models, and attention-based models. this interpretability is one of the most desirable qualities of decision trees. it allows developers to confirm that the model has learned realistic information from the data and allows end-users to have trust and confidence in the decisions made by the model.[37][3] for example, following the path that a decision tree takes to make its decision is quite trivial, but following the paths of tens or hundreds of trees is much harder. to achieve both performance and interpretability, some model compression techniques allow transforming a random forest into a minimal \"born-again\" decision tree that faithfully reproduces the same decision function.[37][47][48] if it is established that the predictive attributes are linearly correlated with the target variable, using random forest may not enhance the accuracy of the base learner.[37][40] furthermore, in problems with multiple categorical variables, random forest may not be able to increase the accuracy of the base learner. data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processes, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.[2]\n","\n","data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).[3] data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.[4]\n","\n","data science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data.[5] it uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.[6] however, data science is different from computer science and information science. turing award winner jim gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.[7][8]\n","\n","a data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.[9]\n","\n","foundations\n","data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. the field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. as such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[11][12] statistician nathan yau, drawing on ben fry, also links data science to human‚Äìcomputer interaction: users should be able to intuitively control and explore data.[13][14] in 2015, the american statistical association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[15]\n","\n","relationship to statistics\n","many statisticians, including nate silver, have argued that data science is not a new field, but rather another name for statistics.[16] others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data.[17] vasant dhar writes that statistics emphasizes quantitative data and description. in contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action.[18] andrew gelman of columbia university has described statistics as a non-essential part of data science.[19]\n","\n","stanford professor david donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. he describes data science as an applied field growing out of traditional statistics.[20]\n","\n","etymology\n","early usage\n","in 1962, john tukey described a field he called \"data analysis\", which resembles modern data science.[20] in 1985, in a lecture given to the chinese academy of sciences in beijing, c. f. jeff wu used the term \"data science\" for the first time as an alternative name for statistics.[21] later, attendees at a 1992 statistics symposium at the university of montpellier  ii acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.[22][23]\n","\n","the term \"data science\" has been traced back to 1974, when peter naur proposed it as an alternative name to computer science.[6] in 1996, the international federation of classification societies became the first conference to specifically feature data science as a topic.[6] however, the definition was still in flux. after the 1985 lecture at the chinese academy of sciences in beijing, in 1997 c. f. jeff wu again suggested that statistics should be renamed data science. he reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data.[24] in 1998, hayashi chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.[23]\n","\n","during the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included \"knowledge discovery\" and \"data mining\".[6][25]\n","\n","modern usage\n","in 2012, technologists thomas h. davenport and dj patil declared \"data scientist: the sexiest job of the 21st century\",[26] a catchphrase that was picked up even by major-city newspapers like the new york times[27] and the boston globe.[28] a decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".[29]\n","\n","the modern conception of data science as an independent discipline is sometimes attributed to william s. cleveland.[30] in a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name.[25] \"data science\" became more widely used in the next few years: in 2002, the committee on data for science and technology launched the data science journal. in 2003, columbia university launched the journal of data science.[25] in 2014, the american statistical association's section on statistical learning and data mining changed its name to the section on statistical learning and data science, reflecting the ascendant popularity of data science.[31]\n","\n","the professional title of \"data scientist\" has been attributed to dj patil and jeff hammerbacher in 2008.[32] though it was used by the national science board in their 2005 report \"long-lived digital data collections: enabling research and education in the 21st century\", it referred broadly to any key role in managing a digital data collection.[33]\n","\n","there is still no consensus on the definition of data science, and it is considered by some to be a buzzword.[34] big data is a related marketing term.[35] data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.[36]\n","\n","data science and data analysis\n","summary statistics and scatterplots showing the datasaurus dozen data set\n","example for the usefulness of exploratory data analysis as demonstrated using the datasaurus dozen data set\n","data science and data analysis are both important disciplines in the field of data management and analysis, but they differ in several key ways. while both fields involve working with data, data science is more of an interdisciplinary field that involves the application of statistical, computational, and machine learning methods to extract insights from data and make predictions, while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.[37][38]\n","\n","data analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. this can involve tasks such as data cleaning, data visualization, and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables. data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. for example, a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.[37]\n","\n","data science, on the other hand, is a more complex and iterative process that involves working with larger, more complex datasets that often require advanced computational and statistical methods to analyze. data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data-driven decisions. in addition to statistical analysis, data science often involves tasks such as data preprocessing, feature engineering, and model selection. for instance, a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences.[38][39]\n","\n","while data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions. data scientists are often responsible for collecting and cleaning data, selecting appropriate analytical techniques, and deploying models in real-world scenarios. they work at the intersection of mathematics, computer science, and domain expertise to solve complex problems and uncover hidden patterns in large datasets.[38]\n","\n","despite these differences, data science and data analysis are closely related fields and often require similar skill sets. both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences. both fields benefit from critical thinking and domain knowledge, as understanding the context and nuances of the data is essential for accurate analysis and modeling.[37][38]\n","\n","in summary, data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis. data analysis focuses on extracting insights and drawing conclusions from structured data, while data science involves a more comprehensive approach that combines statistical analysis, computational methods, and machine learning to extract insights, build predictive models, and drive data-driven decision-making. both fields use data to understand patterns, make informed decisions, and solve complex problems across various domains.\n","\n","cloud computing for data science\n","\n","a cloud-based architecture for enabling big data analytics. data flows from various sources, such as personal computers, laptops, and smart phones, through cloud services for processing and analysis, finally leading to various big data applications.\n","cloud computing can offer access to large amounts of computational power and storage.[40] in big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.[41]\n","\n","some distributed computing frameworks are designed to handle big data workloads. these frameworks can enable data scientists to process and analyze large datasets in parallel, which can reducing processing times.[42]\n","\n","ethical consideration in data science\n","data science involve collecting, processing, and analyzing data which often including personal and sensitive information. ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts [43][44]\n","\n","machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes. in statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.[1][2][3] unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\n","\n","overview\n","supervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem.[4] even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. ensembles combine multiple hypotheses to form a (hopefully) better hypothesis.\n","\n","ensemble learning trains two or more machine learning algorithms to a specific classification or regression task. the algorithms within the ensemble learning model are generally referred as \"base models\", \"base learners\" or \"weak learners\" in literature. the base models can be constructed using a single modelling algorithm or several different algorithms. the idea is train a diverse collection of weak performing models to the same modelling task. as a result, the predicted or classified outcomes of each weak learner have poor predictive ability (high bias, i.e. high model errors) and among the collection of all weak learners the outcome and error values exhibit high variance. fundamentally, an ensemble learning model trains many (at least 2) high-bias (weak) and high-variance (diverse) models to be combined into a stronger and better performing model. essentially, it's a set of algorithmic models ‚Äî which would not produce satisfactory predictive results individually ‚Äî that gets combined or averaged over all base models to produce a single high performing, accurate and low-variance model to fit the task as required.\n","\n","ensemble learning typically refers to bagging (bootstrap-aggregating), boosting or stacking/blending techniques to induce high variability among the base models. bagging creates diversity by generating random samples from the training observations and fitting the same model to each different sample ‚Äî also known as \"homogeneous parallel ensembles\". boosting follows an iterative process by sequentially training each next base model on the up-weighted errors of the previous base model's errors, producing an additive model to reduce the final model errors ‚Äî also known as \"sequential ensemble learning\". stacking or blending consists of different base models, each trained independently (i.e. diverse/high variability) to be combined into the ensemble model ‚Äî producing a \"heterogeneous parallel ensemble\". common applications of ensemble learning include random forests (extension of baggin), boosted tree-models, gradient boosted tree-models and models in applications of stacking are generally more task-specific ‚Äî such as combing clustering techniques with other parametric and/or non-parametric techniques. (see here for a comprehensive overview:[5]\n","\n","the broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.[citation needed]\n","\n","evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model. in one sense, ensemble learning may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. on the other hand, the alternative is to do a lot more learning on one non-ensemble system. an ensemble system may be more efficient at improving overall accuracy for the same increase in compute, storage, or communication resources by using that increase on two or more methods, than would have been improved by increasing resource use for a single method. fast algorithms such as decision trees are commonly used in ensemble methods (for example, random forests), although slower algorithms can benefit from ensemble techniques as well.\n","\n","by analogy, ensemble techniques have been used also in unsupervised learning scenarios, for example in consensus clustering or in anomaly detection.\n","\n","ensemble theory\n","empirically, ensembles tend to yield better results when there is a significant diversity among the models.[6][7] many ensemble methods, therefore, seek to promote diversity among the models they combine.[8][9] although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).[10] using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity.[11] it is possible to increase diversity in the training stage of the model using correlation for regression tasks [12] or using information measures such as cross entropy for classification tasks.[13]\n","\n","\n","an ensemble of classifiers usually has smaller classification error than base models.\n","theoretically, one can justify the diversity concept because the lower bound of the error rate of an ensemble system can be decomposed into accuracy, diversity, and the other term.[14]\n","\n","the geometric framework\n","ensemble learning, including both regression and classification tasks, can be explained using a geometric framework.[15] within this framework, the output of each individual classifier or regressor for the entire dataset can be viewed as a point in a multi-dimensional space. additionally, the target result is also represented as a point in this space, referred to as the \"ideal point.\"\n","\n","the euclidean distance is used as the metric to measure both the performance of a single classifier or regressor (the distance between its point and the ideal point) and the dissimilarity between two classifiers or regressors (the distance between their respective points). this perspective transforms ensemble learning into a deterministic problem.\n","\n","for example, within this geometric framework, it can be proved that the averaging of the outputs (scores) of all base classifiers or regressors can lead to equal or better results than the average of all the individual models. it can also be proved that if the optimal weighting scheme is used, then a weighted averaging approach can outperform any of the individual classifiers or regressors that make up the ensemble or as good as the best performer at least.\n","\n","ensemble size\n","while the number of component classifiers of an ensemble has a great impact on the accuracy of prediction, there is a limited number of studies addressing this problem. a priori determining of ensemble size and the volume and velocity of big data streams make this even more crucial for online ensemble classifiers. mostly statistical tests were used for determining the proper number of components. more recently, a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers would deteriorate the accuracy. it is called \"the law of diminishing returns in ensemble construction.\" their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy.[16][17]\n","\n","common types of ensembles\n","bayes optimal classifier\n","main article: bayes classifier\n","the bayes optimal classifier is a classification technique. it is an ensemble of all the hypotheses in the hypothesis space. on average, no other ensemble can outperform it.[18] the naive bayes classifier is a version of this that assumes that the data is conditionally independent on the class and makes the computation more feasible. each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true. to facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis. the bayes optimal classifier can be expressed with the following equation:\n","\n","ùë¶\n","=\n","a\n","r\n","g\n","m\n","a\n","x\n","ùëê\n","ùëó\n","‚àà\n","ùê∂\n","‚àë\n","‚Ñé\n","ùëñ\n","‚àà\n","ùêª\n","ùëÉ\n","(\n","ùëê\n","ùëó\n","|\n","‚Ñé\n","ùëñ\n",")\n","ùëÉ\n","(\n","ùëá\n","|\n","‚Ñé\n","ùëñ\n",")\n","ùëÉ\n","(\n","‚Ñé\n","ùëñ\n",")\n","{\\displaystyle y={\\underset {c_{j}\\in c}{\\mathrm {argmax} }}\\sum _{h_{i}\\in h}{p(c_{j}|h_{i})p(t|h_{i})p(h_{i})}}\n","where \n","ùë¶\n","{\\displaystyle y} is the predicted class, \n","ùê∂\n","{\\displaystyle c} is the set of all possible classes, \n","ùêª\n","{\\displaystyle h} is the hypothesis space, \n","ùëÉ\n","{\\displaystyle p} refers to a probability, and \n","ùëá\n","{\\displaystyle t} is the training data. as an ensemble, the bayes optimal classifier represents a hypothesis that is not necessarily in \n","ùêª\n","{\\displaystyle h}. the hypothesis represented by the bayes optimal classifier, however, is the optimal hypothesis in ensemble space (the space of all possible ensembles consisting only of hypotheses in \n","ùêª\n","{\\displaystyle h}).\n","\n","this formula can be restated using bayes' theorem, which says that the posterior is proportional to the likelihood times the prior:\n","\n","ùëÉ\n","(\n","‚Ñé\n","ùëñ\n","|\n","ùëá\n",")\n","‚àù\n","ùëÉ\n","(\n","ùëá\n","|\n","‚Ñé\n","ùëñ\n",")\n","ùëÉ\n","(\n","‚Ñé\n","ùëñ\n",")\n","{\\displaystyle p(h_{i}|t)\\propto p(t|h_{i})p(h_{i})}\n","hence,\n","\n","ùë¶\n","=\n","a\n","r\n","g\n","m\n","a\n","x\n","ùëê\n","ùëó\n","‚àà\n","ùê∂\n","‚àë\n","‚Ñé\n","ùëñ\n","‚àà\n","ùêª\n","ùëÉ\n","(\n","ùëê\n","ùëó\n","|\n","‚Ñé\n","ùëñ\n",")\n","ùëÉ\n","(\n","‚Ñé\n","ùëñ\n","|\n","ùëá\n",")\n","{\\displaystyle y={\\underset {c_{j}\\in c}{\\mathrm {argmax} }}\\sum _{h_{i}\\in h}{p(c_{j}|h_{i})p(h_{i}|t)}}\n","bootstrap aggregating (bagging)\n","main article: bootstrap aggregating\n","\n","three datasets bootstrapped from an original set. example a occurs twice in set 1 because these are chosen with replacement.\n","bootstrap aggregation (bagging) involves training an ensemble on bootstrapped data sets. a bootstrapped set is created by selecting from original training data set with replacement. thus, a bootstrap set may contain a given example zero, one, or multiple times. ensemble members can also have limits on the features (e.g., nodes of a decision tree), to encourage exploring of diverse features.[19] the variance of local information in the bootstrap sets and feature considerations promote diversity in the ensemble, and can strengthen the ensemble.[20] to reduce overfitting, a member can be validated using the out-of-bag set (the examples that are not in its bootstrap set).[21]\n","\n","inference is done by voting of predictions of ensemble members, called aggregation. it is illustrated below with an ensemble of four decision trees. the query example is classified by each tree. because three of the four predict the positive class, the ensemble's overall classification is positive. random forests like the one shown are a common application of bagging.\n","\n","an example of the aggregation process for an ensemble of decision trees. individual classifications are aggregated, and an overall classification is derived.\n","boosting\n","main article: boosting (meta-algorithm)\n","boosting involves training successive models by emphasizing training data mis-classified by previously learned models. initially, all data (d1) has equal weight and is used to learn a base model m1. the examples mis-classified by m1 are assigned a weight greater than correctly classified examples. this boosted data (d2) is used to train a second base model m2, and so on. inference is done by voting.\n","\n","in some cases, boosting has yielded better accuracy than bagging, but tends to over-fit more. the most common implementation of boosting is adaboost, but some newer algorithms are reported to achieve better results.[citation needed]\n","\n","bayesian model averaging\n","bayesian model averaging (bma) makes predictions by averaging the predictions of models weighted by their posterior probabilities given the data.[22] bma is known to generally give better answers than a single model, obtained, e.g., via stepwise regression, especially where very different models have nearly identical performance in the training set but may otherwise perform quite differently.\n","\n","the question with any use of bayes' theorem is the prior, i.e., the probability (perhaps subjective) that each model is the best to use for a given purpose. conceptually, bma can be used with any prior. r packages ensemblebma[23] and bma[24] use the prior implied by the bayesian information criterion, (bic), following raftery (1995).[25] r package bas supports the use of the priors implied by akaike information criterion (aic) and other criteria over the alternative models as well as priors over the coefficients.[26]\n","\n","the difference between bic and aic is the strength of preference for parsimony. bic's penalty for model complexity is \n","ln\n","‚Å°\n","(\n","ùëõ\n",")\n","ùëò\n","{\\displaystyle \\ln(n)k} , while aic's is \n","2\n","ùëò\n","{\\displaystyle 2k}. large-sample asymptotic theory establishes that if there is a best model, then with increasing sample sizes, bic is strongly consistent, i.e., will almost certainly find it, while aic may not, because aic may continue to place excessive posterior probability on models that are more complicated than they need to be. on the other hand, aic and aicc are asymptotically \"efficient\" (i.e., minimum mean square prediction error), while bic is not .[27]\n","\n","haussler et al. (1994) showed that when bma is used for classification, its expected error is at most twice the expected error of the bayes optimal classifier.[28] burnham and anderson (1998, 2002) contributed greatly to introducing a wider audience to the basic ideas of bayesian model averaging and popularizing the methodology.[29] the availability of software, including other free open-source packages for r beyond those mentioned above, helped make the methods accessible to a wider audience.[30]\n","\n","bayesian model combination\n","bayesian model combination (bmc) is an algorithmic correction to bayesian model averaging (bma). instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weights drawn randomly from a dirichlet distribution having uniform parameters). this modification overcomes the tendency of bma to converge toward giving all the weight to a single model. although bmc is somewhat more computationally expensive than bma, it tends to yield dramatically better results. bmc has been shown to be better on average (with statistical significance) than bma and bagging.[31]\n","\n","use of bayes' law to compute model weights requires computing the probability of the data given each model. typically, none of the models in the ensemble are exactly the distribution from which the training data were generated, so all of them correctly receive a value close to zero for this term. this would work well if the ensemble were big enough to sample the entire model-space, but this is rarely possible. consequently, each pattern in the training data will cause the ensemble weight to shift toward the model in the ensemble that is closest to the distribution of the training data. it essentially reduces to an unnecessarily complex method for doing model selection.\n","\n","the possible weightings for an ensemble can be visualized as lying on a simplex. at each vertex of the simplex, all of the weight is given to a single model in the ensemble. bma converges toward the vertex that is closest to the distribution of the training data. by contrast, bmc converges toward the point where this distribution projects onto the simplex. in other words, instead of selecting the one model that is closest to the generating distribution, it seeks the combination of models that is closest to the generating distribution.\n","\n","the results from bma can often be approximated by using cross-validation to select the best model from a bucket of models. likewise, the results from bmc may be approximated by using cross-validation to select the best ensemble combination from a random sampling of possible weightings.\n","\n","bucket of models\n","a \"bucket of models\" is an ensemble technique in which a model selection algorithm is used to choose the best model for each problem. when tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set.\n","\n","the most common approach used for model-selection is cross-validation selection (sometimes called a \"bake-off contest\"). it is described with the following pseudo-code:\n","\n","for each model m in the bucket:\n","    do c times: (where 'c' is some constant)\n","        randomly divide the training dataset into two sets: a and b\n","        train m with a\n","        test m with b\n","select the model that obtains the highest average score\n","cross-validation selection can be summed up as: \"try them all with the training set, and pick the one that works best\".[32]\n","\n","gating is a generalization of cross-validation selection. it involves training another learning model to decide which of the models in the bucket is best-suited to solve the problem. often, a perceptron is used for the gating model. it can be used to pick the \"best\" model, or it can be used to give a linear weight to the predictions from each model in the bucket.\n","\n","when a bucket of models is used with a large set of problems, it may be desirable to avoid training some of the models that take a long time to train. landmark learning is a meta-learning approach that seeks to solve this problem. it involves training only the fast (but imprecise) algorithms in the bucket, and then using the performance of these algorithms to help determine which slow (but accurate) algorithm is most likely to do best.[33]\n","\n","amended cross-entropy cost: an approach for encouraging diversity in classification ensemble\n","the most common approach for training classifier is using cross-entropy cost function. however, one would like to train an ensemble of models that have diversity so when we combine them it would provide best results.[34][35] assuming we use a simple ensemble of averaging \n","ùêæ\n","{\\displaystyle k} classifiers. then the amended cross-entropy cost is\n","\n","ùëí\n","ùëò\n","=\n","ùêª\n","(\n","ùëù\n",",\n","ùëû\n","ùëò\n",")\n","‚àí\n","ùúÜ\n","ùêæ\n","‚àë\n","ùëó\n","‚â†\n","ùëò\n","ùêª\n","(\n","ùëû\n","ùëó\n",",\n","ùëû\n","ùëò\n",")\n","{\\displaystyle e^{k}=h(p,q^{k})-{\\frac {\\lambda }{k}}\\sum _{j\\neq k}h(q^{j},q^{k})}\n","where \n","ùëí\n","ùëò\n","{\\displaystyle e^{k}} is the cost function of the \n","ùëò\n","ùë°\n","‚Ñé\n","{\\displaystyle k^{th}} classifier, \n","ùëû\n","ùëò\n","{\\displaystyle q^{k}} is the probability of the \n","ùëò\n","ùë°\n","‚Ñé\n","{\\displaystyle k^{th}} classifier, \n","ùëù\n","{\\displaystyle p} is the true probability that we need to estimate and \n","ùúÜ{\\displaystyle \\lambda } is a parameter between 0 and 1 that define the diversity that we would like to establish. when \n","ùúÜ\n","=\n","0\n","{\\displaystyle \\lambda =0} we want each classifier to do its best regardless of the ensemble and when \n","ùúÜ\n","=\n","1\n","{\\displaystyle \\lambda =1} we would like the classifier to be as diverse as possible.\n","\n","stacking\n","stacking (sometimes called stacked generalization) involves training a model to combine the predictions of several other learning algorithms. first, all of the other algorithms are trained using the available data, then a combiner algorithm (final estimator) is trained to make a final prediction using all the predictions of the other algorithms (base estimators) as additional inputs or using cross-validated predictions from the base estimators which can prevent overfitting.[36] if an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although, in practice, a logistic regression model is often used as the combiner.\n","\n","stacking typically yields performance better than any single one of the trained models.[37] it has been successfully used on both supervised learning tasks (regression,[38] classification and distance learning [39]) and unsupervised learning (density estimation).[40] it has also been used to estimate bagging's error rate.[3][41] it has been reported to out-perform bayesian model-averaging.[42] the two top-performers in the netflix competition utilized blending, which may be considered a form of stacking.[43]\n","\n","voting\n","voting is another form of ensembling. see e.g. weighted majority algorithm (machine learning).\n","\n","implementations in statistics packages\n","r: at least three packages offer bayesian model averaging tools,[44] including the bms (an acronym for bayesian model selection) package,[45] the bas (an acronym for bayesian adaptive sampling) package,[46] and the bma package.[47]\n","python: scikit-learn, a package for machine learning in python offers packages for ensemble learning including packages for bagging, voting and averaging methods.\n","matlab: classification ensembles are implemented in statistics and machine learning toolbox.[48]\n","ensemble learning applications\n","in recent years, due to growing computational power, which allows for training in large ensemble learning in a reasonable time frame, the number of ensemble learning applications has grown increasingly.[49] some of the applications of ensemble classifiers include:\n","\n","remote sensing\n","main article: remote sensing\n","land cover mapping\n","land cover mapping is one of the major applications of earth observation satellite sensors, using remote sensing and geospatial data, to identify the materials and objects which are located on the surface of target areas. generally, the classes of target materials include roads, buildings, rivers, lakes, and vegetation.[50] some different ensemble learning approaches based on artificial neural networks,[51] kernel principal component analysis (kpca),[52] decision trees with boosting,[53] random forest[50][54] and automatic design of multiple classifier systems,[55] are proposed to efficiently identify land cover objects.\n","\n","change detection\n","change detection is an image analysis problem, consisting of the identification of places where the land cover has changed over time. change detection is widely used in fields such as urban growth, forest and vegetation dynamics, land use and disaster monitoring.[56] the earliest applications of ensemble classifiers in change detection are designed with the majority voting,[57] bayesian model averaging,[58] and the maximum posterior probability.[59] given the growth of satellite data over time, the past decade sees more use of time series methods for continuous change detection from image stacks.[60] one example is a bayesian ensemble changepoint detection method called beast, with the software available as a package rbeast in r, python, and matlab.[61]\n","\n","computer security\n","distributed denial of service\n","distributed denial of service is one of the most threatening cyber-attacks that may happen to an internet service provider.[49] by combining the output of single classifiers, ensemble classifiers reduce the total error of detecting and discriminating such attacks from legitimate flash crowds.[62]\n","\n","malware detection\n","classification of malware codes such as computer viruses, computer worms, trojans, ransomware and spywares with the usage of machine learning techniques, is inspired by the document categorization problem.[63] ensemble learning systems have shown a proper efficacy in this area.[64][65]\n","\n","intrusion detection\n","an intrusion detection system monitors computer network or computer systems to identify intruder codes like an anomaly detection process. ensemble learning successfully aids such monitoring systems to reduce their total error.[66][67]\n","\n","face recognition\n","main article: face recognition\n","face recognition, which recently has become one of the most popular research areas of pattern recognition, copes with identification or verification of a person by their digital images.[68]\n","\n","hierarchical ensembles based on gabor fisher classifier and independent component analysis preprocessing techniques are some of the earliest ensembles employed in this field.[69][70][71]\n","\n","emotion recognition\n","main article: emotion recognition\n","while speech recognition is mainly based on deep learning because most of the industry players in this field like google, microsoft and ibm reveal that the core technology of their speech recognition is based on this approach, speech-based emotion recognition can also have a satisfactory performance with ensemble learning.[72][73]\n","\n","it is also being successfully used in facial emotion recognition.[74][75][76]\n","\n","fraud detection\n","main article: fraud detection\n","fraud detection deals with the identification of bank fraud, such as money laundering, credit card fraud and telecommunication fraud, which have vast domains of research and applications of machine learning. because ensemble learning improves the robustness of the normal behavior modelling, it has been proposed as an efficient technique to detect such fraudulent cases and activities in banking and credit card systems.[77][78]\n","\n","financial decision-making\n","the accuracy of prediction of business failure is a very crucial issue in financial decision-making. therefore, different ensemble classifiers are proposed to predict financial crises and financial distress.[79] also, in the trade-based manipulation problem, where traders attempt to manipulate stock prices by buying and selling activities, ensemble classifiers are required to analyze the changes in the stock market data and detect suspicious symptom of stock price manipulation.[79]\n","\n","medicine\n","ensemble classifiers have been successfully applied in neuroscience, proteomics and medical diagnosis like in neuro-cognitive disorder (i.e. alzheimer or myotonic dystrophy) detection based on mri datasets,[80][81][82] and cervical cytology classification. in mathematics and computer science, an algorithm (/Àà√¶l…°…ôr…™√∞…ôm/ ‚ìò) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation.[1] algorithms are used as specifications for performing calculations and data processing. more advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. using human characteristics as descriptors of machines in metaphorical ways was already practiced by alan turing with terms such as \"memory\", \"search\" and \"stimulus\".[2]\n","\n","in contrast, a heuristic is an approach to problem-solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.[3] for example, social media recommender systems rely on heuristics in such a way that, although widely characterized as \"algorithms\" in 21st-century popular media, cannot deliver correct results due to the nature of the problem.\n","\n","as an effective method, an algorithm can be expressed within a finite amount of space and time[4] and in a well-defined formal language[5] for calculating a function.[6] starting from an initial state and initial input (perhaps empty),[7] the instructions describe a computation that, when executed, proceeds through a finite[8] number of well-defined successive states, eventually producing \"output\"[9] and terminating at a final ending state. the transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[10]\n","\n","etymology\n","around 825 ad, persian scientist and polymath mu·∏•ammad ibn m≈´sƒÅ al-khwƒÅrizmƒ´ wrote kitƒÅb al-·∏•isƒÅb al-hindƒ´ (\"book of indian computation\") and kitab al-jam' wa'l-tafriq al-·∏•isƒÅb al-hindƒ´ (\"addition and subtraction in indian arithmetic\").[1] in the early 12th century, latin translations of said al-khwarizmi texts involving the hindu‚Äìarabic numeral system and arithmetic appeared, for example liber alghoarismi de practica arismetrice, attributed to john of seville, and liber algorismi de numero indorum, attributed to adelard of bath.[2] hereby, alghoarismi or algorismi is the latinization of al-khwarizmi's name; the text starts with the phrase dixit algorismi, or \"thus spoke al-khwarizmi\".[3] around 1230, the english word algorism is attested and then by chaucer in 1391, english adopted the french term.[4][5][clarification needed] in the 15th century, under the influence of the greek word ·ºÄœÅŒπŒ∏ŒºœåœÇ (arithmos, \"number\"; cf. \"arithmetic\"), the latin word was altered to algorithmus.[citation needed]\n","\n","definition\n","for a detailed presentation of the various points of view on the definition of \"algorithm\", see algorithm characterizations.\n","one informal definition is \"a set of rules that precisely defines a sequence of operations\",[11][need quotation to verify] which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure[12] or cook-book recipe.[13] in general, a program is an algorithm only if it stops eventually[14]‚Äîeven though infinite loops may sometimes prove desirable. boolos, jeffrey & 1974, 1999 define an algorithm to be a set of instructions for determining an output, given explicitly, in a form that can be followed by either a computing machine or a human who could only carry out specific elementary operations on symbols.[15]\n","\n","the concept of algorithm is also used to define the notion of decidability‚Äîa notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. in logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. from such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.\n","\n","most algorithms are intended to be implemented as computer programs. however, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\n","\n","history\n","\n","this section is missing information about 20th and 21st century development of computer algorithms. please expand the section to include this information. further details may exist on the talk page. (october 2023)\n","ancient algorithms\n","since antiquity, step-by-step procedures for solving mathematical problems have been attested. this includes in babylonian mathematics (around 2500 bc),[16] egyptian mathematics (around 1550 bc),[16] indian mathematics (around 800 bc and later),[17][18] the ifa oracle (around 500 bc), greek mathematics (around 240 bc),[19] and arabic mathematics (around 800 ad).[20]\n","\n","the earliest evidence of algorithms is found in the babylonian mathematics of ancient mesopotamia (modern iraq). a sumerian clay tablet found in shuruppak near baghdad and dated to c.‚Äâ2500 bc described the earliest division algorithm.[16] during the hammurabi dynasty c.‚Äâ1800 ‚Äì c.‚Äâ1600 bc, babylonian clay tablets described algorithms for computing formulas.[21] algorithms were also used in babylonian astronomy. babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.[22]\n","\n","algorithms for arithmetic are also found in ancient egyptian mathematics, dating back to the rhind mathematical papyrus c.‚Äâ1550 bc.[16] algorithms were later used in ancient hellenistic mathematics. two examples are the sieve of eratosthenes, which was described in the introduction to arithmetic by nicomachus,[23][19]:‚Ääch 9.2‚Ää and the euclidean algorithm, which was first described in euclid's elements (c.‚Äâ300 bc).[19]:‚Ääch 9.1‚Ääexamples of ancient indian mathematics included the shulba sutras, the kerala school, and the brƒÅhmasphu·π≠asiddhƒÅnta.[17]\n","\n","the first cryptographic algorithm for deciphering encrypted code was developed by al-kindi, a 9th-century arab mathematician, in a manuscript on deciphering cryptographic messages. he gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.[20]\n","\n","computers\n","weight-driven clocks\n","bolter credits the invention of the weight-driven clock as \"the key invention [of europe in the middle ages]\". in particular, he credits the verge escapement mechanism[24] that provides us with the tick and tock of a mechanical clock. \"the accurate automatic machine\"[25] led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"‚Äîthe difference engine and analytical engines of charles babbage and countess ada lovelace, mid-19th century.[26] lovelace is credited with the first creation of an algorithm intended for processing on a computer‚Äîbabbage's analytical engine, the first device considered a real turing-complete computer instead of just a calculator‚Äîand is sometimes called \"history's first programmer\" as a result, though a full implementation of babbage's second device would not be realized until decades after her lifetime.\n","\n","electromechanical relay\n","bell and newell (1971) indicate that the jacquard loom (1801), a precursor to hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers.[27] by the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. by the late 19th century, the ticker tape (c.‚Äâ1870s) was in use, as was the use of hollerith cards in the 1890 u.s. census. then came the teleprinter (c.‚Äâ1910) with its punched-paper use of baudot code on tape.\n","\n","telephone-switching networks of electromechanical relays (invented 1835) was behind the work of george stibitz (1937), the inventor of the digital adding device. as he worked in bell laboratories, he observed the \"burdensome' use of mechanical calculators with gears. \"he went home one evening in 1937 intending to test his idea... when the tinkering was over, stibitz had constructed a binary adding device\".[28] the mathematician martin davis supported the particular importance of the electromechanical relay.[29]\n","\n","formalization\n","\n","ada lovelace's diagram from \"note g\", the first published computer algorithm\n","in 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the entscheidungsproblem (decision problem) posed by david hilbert. later formalizations were framed as attempts to define \"effective calculability\"[30] or \"effective method\".[31] those formalizations included the g√∂del‚Äìherbrand‚Äìkleene recursive functions of 1930, 1934 and 1935, alonzo church's lambda calculus of 1936, emil post's formulation 1 of 1936, and alan turing's turing machines of 1936‚Äì37 and 1939.\n","\n","representations\n","algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. pseudocode, flowcharts, drakon-charts, and control tables are structured ways to express algorithms that avoid many of the ambiguities common in statements based on natural language. programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but they are also often used as a way to define or document algorithms.\n","\n","turing machines\n","there is a wide variety of representations possible and one can express a given turing machine program as a sequence of machine tables (see finite-state machine, state-transition table, and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see turing machine for more). representations of algorithms can also be classified into three accepted levels of turing machine description: high-level description, implementation description, and formal description.[32] a high-level description describes qualities of the algorithm itself, ignoring how it is implemented on the turing machine.[32] an implementation description describes the general manner in which the machine moves its head and stores data in order to carry out the algorithm, but does not give exact states.[32] in the most detail, a formal description gives the exact state table and list of transitions of the turing machine.[32]\n","\n","flowchart representation\n","the graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). like the program flow of a minsky machine, a flowchart always starts at the top of a page and proceeds down. its primary symbols are only four: the directed arrow showing program flow, the rectangle (sequence, goto), the diamond (if-then-else), and the dot (or-tie). the b√∂hm‚Äìjacopini canonical structures are made of these primitive shapes. sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. the symbols and their use to build the canonical structures are shown in the diagram.[33]\n","\n","algorithmic analysis\n","main article: analysis of algorithms\n","it is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of ‚Å†\n","ùëÇ\n","(\n","ùëõ\n",")\n","{\\displaystyle o(n)}‚Å†, using big o notation. at all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. therefore, it is said to have a space requirement of ‚Å†\n","ùëÇ\n","(\n","1\n",")\n","{\\displaystyle o(1)}‚Å†, if the space required to store the input numbers is not counted, or ‚Å†\n","ùëÇ\n","(\n","ùëõ\n",")\n","{\\displaystyle o(n)}‚Å† if it is counted.\n","\n","different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. for example, a binary search algorithm (with cost ‚Å†\n","ùëÇ\n","(\n","log\n","‚Å°\n","ùëõ\n",")\n","{\\displaystyle o(\\log n)}‚Å†) outperforms a sequential search (cost ‚Å†\n","ùëÇ\n","(\n","ùëõ\n",")\n","{\\displaystyle o(n)}‚Å† ) when used for table lookups on sorted lists or arrays.\n","\n","formal versus empirical\n","main articles: empirical algorithmics, profiling (computer programming), and program optimization\n","the analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. in this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. usually, pseudocode is used for analysis as it is the simplest and most general representation. however, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. for the solution of a \"one-off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\n","\n","empirical testing is useful because it may uncover unexpected interactions that affect performance. benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization. empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.[34]\n","\n","execution efficiency\n","main article: algorithmic efficiency\n","to illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to fft algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[35] in general, speed improvements depend on special properties of the problem, which are very common in practical applications.[36] speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n","\n","design\n","see also: algorithm ¬ß by design paradigm\n","algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. the design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. techniques for designing and implementing algorithm designs are also called algorithm design patterns,[37] with examples including the template method pattern and the decorator pattern. one of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big o notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases.[38]\n","\n","structured programming\n","per the church‚Äìturing thesis, any algorithm can be computed by a model known to be turing complete. in fact, it has been demonstrated that turing completeness requires only four instruction types‚Äîconditional goto, unconditional goto, assignment, halt. however, kemeny and kurtz observe that, while \"undisciplined\" use of unconditional gotos and conditional if-then gotos can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\".[39] tausworthe augments the three b√∂hm-jacopini canonical structures:[40] sequence, if-then-else, and while-do, with two more: do-while and case.[41] an additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.[42]\n","\n","legal status\n","see also: software patent\n","algorithms, by themselves, are not usually patentable. in the united states, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (uspto 2006), so algorithms are not patentable (as in gottschalk v. benson). however practical applications of algorithms are sometimes patentable. for example, in diamond v. diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. the patenting of software is controversial,[43] and there are criticized patents involving algorithms, especially data compression algorithms, such as unisys's lzw patent. additionally, some cryptographic algorithms have export restrictions (see export of cryptography).\n","\n","classification\n","there are various ways to classify algorithms, each with its own merits.\n","\n","by implementation\n","one way to classify algorithms is by implementation means.\n","\n","int gcd(int a, int b) {\n","    if (b == 0)\n","        return a;\n","    else if (a > b)\n","        return gcd(a-b,b);\n","    else\n","        return gcd(a,b-a);\n","}\n","recursive c implementation of euclid's algorithm from the above flowchart\n","recursion\n","a recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. some problems are naturally suited for one implementation or the other. for example, towers of hanoi is well understood using recursive implementation. every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\n","serial, parallel or distributed\n","algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. those computers are sometimes called serial computers. an algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. parallel algorithms are algorithms that take advantage of computer architectures where multiple processors can work on a problem at the same time. distributed algorithms are algorithms that use multiple machines connected to a computer network. parallel and distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. for example, a cpu would be an example of a parallel algorithm. the resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.\n","deterministic or non-deterministic\n","deterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.\n","exact or approximate\n","while many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. the approximation can be reached by either using a deterministic or a random strategy. such algorithms have practical value for many hard problems. one of the examples of an approximate algorithm is the knapsack problem, where there is a set of given items. its goal is to pack the knapsack to get the maximum total value. each item has some weight and some value. the total weight that can be carried is no more than some fixed number x. so, the solution must consider weights of items as well as their value.[44]\n","quantum algorithm\n","quantum algorithms run on a realistic model of quantum computation. the term is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum computing such as quantum superposition or quantum entanglement.\n","by design paradigm\n","another way of classifying algorithms is by their design methodology or paradigm. there is a certain number of paradigms, each different from the other. furthermore, each of these categories includes many different types of algorithms. some common paradigms are:\n","\n","brute-force or exhaustive search\n","brute force is a method of problem-solving that involves systematically trying every possible option until the optimal solution is found. this approach can be very time-consuming, as it requires going through every possible combination of variables. however, it is often used when other methods are not available or too complex. brute force can be used to solve a variety of problems, including finding the shortest path between two points and cracking passwords.\n","divide and conquer\n","a divide-and-conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. one such example of divide and conquer is merge sorting. sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. a simpler variant of divide and conquer is called a decrease-and-conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. an example of a decrease and conquer algorithm is the binary search algorithm.\n","search and enumeration\n","many problems (such as playing chess) can be modeled as problems on graphs. a graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. this category also includes search algorithms, branch and bound enumeration, and backtracking.\n","randomized algorithm\n","such algorithms make some choices randomly (or pseudo-randomly). they can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). for some of these problems, it is known that the fastest approximations must involve some randomness.[45] whether randomized algorithms with polynomial time complexity can be the fastest algorithm for some problems is an open question known as the p versus np problem. there are two large classes of such algorithms:\n","monte carlo algorithms return a correct answer with high-probability. e.g. rp is the subclass of these that run in polynomial time.\n","las vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. zpp.\n","reduction of complexity\n","this technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. the goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithms. for example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). this technique is also known as transform and conquer.\n","back tracking\n","in this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.\n","optimization problems\n","for optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n","\n","linear programming\n","when searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. there are algorithms that can solve any problem in this category, such as the popular simplex algorithm.[46] problems that can be solved with linear programming include the maximum flow problem for directed graphs. if a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. a linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. in the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.\n","dynamic programming\n","when a problem shows optimal substructures‚Äîmeaning the optimal solution to a problem can be constructed from optimal solutions to subproblems‚Äîand overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. for example, floyd‚Äìwarshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. dynamic programming and memoization go together. the main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. the difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. when subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. by using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.\n","the greedy method\n","a greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. for some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. the most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. huffman tree, kruskal, prim, sollin are greedy algorithms that can solve this optimization problem.\n","the heuristic method\n","in optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. these algorithms work by getting closer and closer to the optimal solution as they progress. in principle, if run for an infinite amount of time, they will find the optimal solution. their merit is that they can find a solution very close to the optimal solution in a relatively short time. such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. when a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.\n","examples\n","further information: list of algorithms\n","one of the simplest algorithms is to find the largest number in a list of numbers of random order. finding the solution requires looking at every number in the list. from this follows a simple algorithm, which can be stated in a high-level description in english prose, as:\n","\n","high-level description:\n","\n","if there are no numbers in the set, then there is no highest number.\n","assume the first number in the set is the largest number in the set.\n","for each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.\n","when there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.\n","(quasi-)formal description: written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n","\n","algorithm largestnumber\n","input: a list of numbers l.\n","output: the largest number in the list l.\n","if l.size = 0 return null\n","largest ‚Üê l[0]\n","for each item in l, do\n","    if item > largest, then\n","        largest ‚Üê item\n","return largest\n","\"‚Üê\" denotes assignment. for instance, \"largest ‚Üê item\" means that the value of largest changes to the value of item.\n","\"return\" terminates the algorithm and outputs the following value. a decision tree is a decision support hierarchical model that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. it is one way to display an algorithm that only contains conditional control statements.\n","\n","decision trees are commonly used in operations research, specifically in decision analysis,[1] to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.\n","\n","overview\n","a decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). the paths from root to leaf represent classification rules.\n","\n","in decision analysis, a decision tree and the closely related influence diagram are used as a visual and analytical decision support tool, where the expected values (or expected utility) of competing alternatives are calculated.\n","\n","a decision tree consists of three types of nodes:[2]\n","\n","decision nodes ‚Äì typically represented by squares\n","chance nodes ‚Äì typically represented by circles\n","end nodes ‚Äì typically represented by triangles\n","decision trees are commonly used in operations research and operations management. if, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm.[citation needed] another use of decision trees is as a descriptive means for calculating conditional probabilities.\n","\n","decision trees, influence diagrams, utility functions, and other decision analysis tools and methods are taught to undergraduate students in schools of business, health economics, and public health, and are examples of operations research or management science methods. these tools are also used to predict decisions of householders in normal and emergency scenarios.[3][4]\n","\n","decision-tree building blocks\n","decision-tree elements\n","\n","drawn from left to right, a decision tree has only burst nodes (splitting paths) but no sink nodes (converging paths). so used manually they can grow very big and are then often hard to draw fully by hand. traditionally, decision trees have been created manually ‚Äì as the aside example shows ‚Äì although increasingly, specialized software is employed.\n","\n","decision rules\n","the decision tree can be linearized into decision rules,[5] where the outcome is the contents of the leaf node, and the conditions along the path form a conjunction in the if clause. in general, the rules have the form:\n","\n","if condition1 and condition2 and condition3 then outcome.\n","decision rules can be generated by constructing association rules with the target variable on the right. they can also denote temporal or causal relations.[6]\n","\n","decision tree using flowchart symbols\n","commonly a decision tree is drawn using flowchart symbols as it is easier for many to read and understand. note there is a conceptual error in the \"proceed\" calculation of the tree shown below; the error relates to the calculation of \"costs\" awarded in a legal action.\n","\n","\n","analysis example\n","analysis can take into account the decision maker's (e.g., the company's) preference or utility function, for example:\n","\n","\n","the basic interpretation in this situation is that the company prefers b's risk and payoffs under realistic risk preference coefficients (greater than $400k‚Äîin that range of risk aversion, the company would need to model a third strategy, \"neither a nor b\").\n","\n","another example, commonly used in operations research courses, is the distribution of lifeguards on beaches (a.k.a. the \"life's a beach\" example).[7] the example describes two beaches with lifeguards to be distributed on each beach. there is maximum budget b that can be distributed among the two beaches (in total), and using a marginal returns table, analysts can decide how many lifeguards to allocate to each beach.\n","\n","lifeguards on each beach\tdrownings prevented in total, beach #1\tdrownings prevented in total, beach #2\n","1\t3\t1\n","2\t0\t4\n","in this example, a decision tree can be drawn to illustrate the principles of diminishing returns on beach #1.\n","\n","\n","beach decision tree\n","the decision tree illustrates that when sequentially distributing lifeguards, placing a first lifeguard on beach #1 would be optimal if there is only the budget for 1 lifeguard. but if there is a budget for two guards, then placing both on beach #2 would prevent more overall drownings.\n","\n","\n","lifeguards\n","influence diagram\n","much of the information in a decision tree can be represented more compactly as an influence diagram, focusing attention on the issues and relationships between events.\n","\n","\n","the rectangle on the left represents a decision, the ovals represent actions, and the diamond represents results.\n","association rule induction\n","main article: decision tree learning\n","decision trees can also be seen as generative models of induction rules from empirical data. an optimal decision tree is then defined as a tree that accounts for most of the data, while minimizing the number of levels (or \"questions\").[8] several algorithms to generate such optimal trees have been devised, such as id3/4/5,[9] cls, assistant, and cart.\n","\n","advantages and disadvantages\n","among decision support tools, decision trees (and influence diagrams) have several advantages. decision trees:\n","\n","are simple to understand and interpret. people are able to understand decision tree models after a brief explanation.\n","have value even with little hard data. important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes.\n","help determine worst, best, and expected values for different scenarios.\n","use a white box model. if a given result is provided by a model.\n","can be combined with other decision techniques.\n","the action of more than one decision-maker can be considered.\n","disadvantages of decision trees:\n","\n","they are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree.\n","they are often relatively inaccurate. many other predictors perform better with similar data. this can be remedied by replacing a single decision tree with a random forest of decision trees, but a random forest is not as easy to interpret as a single decision tree.\n","for data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of those attributes with more levels.[10]\n","calculations can get very complex, particularly if many values are uncertain and/or if many outcomes are linked.\n","optimizing a decision tree\n","a few things should be considered when improving the accuracy of the decision tree classifier. the following are some possible optimizations to consider when looking to make sure the decision tree model produced makes the correct decision or classification. note that these things are not the only things to consider but only some.\n","\n","increasing the number of levels of the tree\n","\n","the accuracy of the decision tree can change based on the depth of the decision tree. in many cases, the tree‚Äôs leaves are pure nodes.[11] when a node is pure, it means that all the data in that node belongs to a single class.[12] for example, if the classes in the data set are cancer and non-cancer a leaf node would be considered pure when all the sample data in a leaf node is part of only one class, either cancer or non-cancer. it is important to note that a deeper tree is not always better when optimizing the decision tree. a deeper tree can influence the runtime in a negative way. if a certain classification algorithm is being used, then a deeper tree could mean the runtime of this classification algorithm is significantly slower. there is also the possibility that the actual algorithm building the decision tree will get significantly slower as the tree gets deeper. if the tree-building algorithm being used splits pure nodes, then a decrease in the overall accuracy of the tree classifier could be experienced. occasionally, going deeper in the tree can cause an accuracy decrease in general, so it is very important to test modifying the depth of the decision tree and selecting the depth that produces the best results. to summarize, observe the points below, we will define the number d as the depth of the tree.\n","\n","possible advantages of increasing the number d:\n","\n","accuracy of the decision-tree classification model increases.\n","possible disadvantages of increasing d\n","\n"," runtime issues\n","decrease in accuracy in general\n","pure node splits while going deeper can cause issues.\n","the ability to test the differences in classification results when changing d is imperative. we must be able to easily change and test the variables that could affect the accuracy and reliability of the decision tree-model.\n","\n","the choice of node-splitting functions\n","\n","the node splitting function used can have an impact on improving the accuracy of the decision tree. for example, using the information-gain function may yield better results than using the phi function. the phi function is known as a measure of ‚Äúgoodness‚Äù of a candidate split at a node in the decision tree. the information gain function is known as a measure of the ‚Äúreduction in entropy‚Äù. in the following, we will build two decision trees. one decision tree will be built using the phi function to split the nodes and one decision tree will be built using the information gain function to split the nodes.\n","\n","the main advantages and disadvantages of information gain and phi function\n","\n","one major drawback of information gain is that the feature that is chosen as the next node in the tree tends to have more unique values.[13]\n","an advantage of information gain is that it tends to choose the most impactful features that are close to the root of the tree. it is a very good measure for deciding the relevance of some features.\n","the phi function is also a good measure for deciding the relevance of some features based on \"goodness\".\n","this is the information gain function formula. the formula states the information gain is a function of the entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree.\n","\n","ùêº\n","gain\n","(\n","ùë†\n",")\n","=\n","ùêª\n","(\n","ùë°\n",")\n","‚àí\n","ùêª\n","(\n","ùë†\n",",\n","ùë°\n",")\n","{\\displaystyle i_{\\textrm {gain}}(s)=h(t)-h(s,t)}\n","\n","this is the phi function formula. the phi function is maximized when the chosen feature splits the samples in a way that produces homogenous splits and have around the same number of samples in each split.\n","\n","œÜ\n","(\n","ùë†\n",",\n","ùë°\n",")\n","=\n","(\n","2\n","‚àó\n","ùëÉ\n","ùêø\n","‚àó\n","ùëÉ\n","ùëÖ\n",")\n","‚àó\n","ùëÑ\n","(\n","ùë†\n","|\n","ùë°\n",")\n","{\\displaystyle \\phi (s,t)=(2*p_{l}*p_{r})*q(s|t)}\n","\n","we will set d, which is the depth of the decision tree we are building, to three (d = 3). we also have the following data set of cancer and non-cancer samples and the mutation features that the samples either have or do not have. if a sample has a feature mutation then the sample is positive for that mutation, and it will be represented by one. if a sample does not have a feature mutation then the sample is negative for that mutation, and it will be represented by zero.\n","\n","to summarize, c stands for cancer and nc stands for non-cancer. the letter m stands for mutation, and if a sample has a particular mutation it will show up in the table as a one and otherwise zero.\n","\n","the sample data\n","m1\tm2\tm3\tm4\tm5\n","c1\t0\t1\t0\t1\t1\n","nc1\t0\t0\t0\t0\t0\n","nc2\t0\t0\t1\t1\t0\n","nc3\t0\t0\t0\t0\t0\n","c2\t1\t1\t1\t1\t1\n","nc4\t0\t0\t0\t1\t0\n","now, we can use the formulas to calculate the phi function values and information gain values for each m in the dataset. once all the values are calculated the tree can be produced. the first thing to be done is to select the root node. in information gain and the phi function we consider the optimal split to be the mutation that produces the highest value for information gain or the phi function. now assume that m1 has the highest phi function value and m4 has the highest information gain value. the m1 mutation will be the root of our phi function tree and m4 will be the root of our information gain tree. you can observe the root nodes below\n","\n","figure 1: the left node is the root node of the tree we are building using the phi function to split the nodes. the right node is the root node of the tree we are building using information gain to split the nodes.\n","now, once we have chosen the root node we can split the samples into two groups based on whether a sample is positive or negative for the root node mutation. the groups will be called group a and group b. for example, if we use m1 to split the samples in the root node we get nc2 and c2 samples in group a and the rest of the samples nc4, nc3, nc1, c1 in group b.\n","\n","disregarding the mutation chosen for the root node, proceed to place the next best features that have the highest values for information gain or the phi function in the left or right child nodes of the decision tree. once we choose the root node and the two child nodes for the tree of depth = 3 we can just add the leaves. the leaves will represent the final classification decision the model has produced based on the mutations a sample either has or does not have. the left tree is the decision tree we obtain from using information gain to split the nodes and the right tree is what we obtain from using the phi function to split the nodes.\n","\n","the resulting tree from using information gain to split the nodes\n","\n","now assume the classification results from both trees are given using a confusion matrix.\n","\n","information gain confusion matrix:\n","\n","predicted: c\tpredicted: nc\n","actual: c\t1\t1\n","actual: nc\t0\t4\n","phi function confusion matrix:\n","\n","predicted: c\tpredicted: nc\n","actual: c\t2\t0\n","actual: nc\t1\t3\n","the tree using information gain has the same results when using the phi function when calculating the accuracy. when we classify the samples based on the model using information gain we get one true positive, one false positive, zero false negatives, and four true negatives. for the model using the phi function we get two true positives, zero false positives, one false negative, and three true negatives. the next step is to evaluate the effectiveness of the decision tree using some key metrics that will be discussed in the evaluating a decision tree section below. the metrics that will be discussed below can help determine the next steps to be taken when optimizing the decision tree.\n","\n","other techniques\n","\n","the above information is not where it ends for building and optimizing a decision tree. there are many techniques for improving the decision tree classification models we build. one of the techniques is making our decision tree model from a bootstrapped dataset. the bootstrapped dataset helps remove the bias that occurs when building a decision tree model with the same data the model is tested with. the ability to leverage the power of random forests can also help significantly improve the overall accuracy of the model being built. this method generates many decisions from many decision trees and tallies up the votes from each decision tree to make the final classification. there are many techniques, but the main objective is to test building your decision tree model in different ways to make sure it reaches the highest performance level possible.\n","\n","evaluating a decision tree\n","it is important to know the measurements used to evaluate decision trees. the main metrics used are accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate. all these measurements are derived from the number of true positives, false positives, true negatives, and false negatives obtained when running a set of samples through the decision tree classification model. also, a confusion matrix can be made to display these results. all these main metrics tell something different about the strengths and weaknesses of the classification model built based on your decision tree. for example, a low sensitivity with high specificity could indicate the classification model built from the decision tree does not do well identifying cancer samples over non-cancer samples.\n","\n","let us take the confusion matrix below. the confusion matrix shows us the decision tree model classifier built gave 11 true positives, 1 false positive, 45 false negatives, and 105 true negatives.\n","\n","predicted: c\tpredicted: nc\n","actual: c\t11\t45\n","actual: nc\t1\t105\n","we will now calculate the values accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate.\n","\n","accuracy:\n","\n","ùê¥\n","ùëê\n","ùëê\n","ùë¢\n","ùëü\n","ùëé\n","ùëê\n","ùë¶\n","=\n","(\n","ùëá\n","ùëÉ\n","+\n","ùëá\n","ùëÅ\n",")\n","/\n","(\n","ùëá\n","ùëÉ\n","+\n","ùëá\n","ùëÅ\n","+\n","ùêπ\n","ùëÉ\n","+\n","ùêπ\n","ùëÅ\n",")\n","{\\displaystyle accuracy=(tp+tn)/(tp+tn+fp+fn)}\n","\n","(\n","11\n","+\n","105\n",")\n","√∑\n","162\n","=\n","71.60\n","%{\\displaystyle (11+105)\\div 162=71.60\\%}\n","\n","sensitivity (tpr ‚Äì true positive rate):[14]\n","\n","ùëá\n","ùëÉ\n","ùëÖ\n","=\n","ùëá\n","ùëÉ\n","/\n","(\n","ùëá\n","ùëÉ\n","+\n","ùêπ\n","ùëÅ\n",")\n","{\\displaystyle tpr=tp/(tp+fn)}\n","\n","(\n","11\n",")\n","√∑\n","(\n","11\n","+\n","45\n",")\n","=\n","19.64\n","%{\\displaystyle (11)\\div (11+45)=19.64\\%}\n","\n","specificity (tnr ‚Äì true negative rate):\n","\n","ùëá\n","ùëÅ\n","ùëÖ\n","=\n","ùëá\n","ùëÅ\n","/\n","(\n","ùëá\n","ùëÅ\n","+\n","ùêπ\n","ùëÉ\n",")\n","{\\displaystyle tnr=tn/(tn+fp)}\n","\n","105\n","√∑\n","(\n","105\n","+\n","1\n",")\n","=\n","99.06\n","%{\\displaystyle 105\\div (105+1)=99.06\\%}\n","\n","precision (ppv ‚Äì positive predictive value):\n","\n","ùëÉ\n","ùëÉ\n","ùëâ\n","=\n","ùëá\n","ùëÉ\n","/\n","(\n","ùëá\n","ùëÉ\n","+\n","ùêπ\n","ùëÉ\n",")\n","{\\displaystyle ppv=tp/(tp+fp)}\n","\n","11\n","/\n","(\n","11\n","+\n","1\n",")\n","=\n","91.66\n","%{\\displaystyle 11/(11+1)=91.66\\%}\n","\n","miss rate (fnr ‚Äì false negative rate):\n","\n","ùêπ\n","ùëÅ\n","ùëÖ\n","=\n","ùêπ\n","ùëÅ\n","/\n","(\n","ùêπ\n","ùëÅ\n","+\n","ùëá\n","ùëÉ\n",")\n","{\\displaystyle fnr=fn/(fn+tp)}\n","\n","45\n","√∑\n","(\n","45\n","+\n","11\n",")\n","=\n","80.35\n","%{\\displaystyle 45\\div (45+11)=80.35\\%}\n","\n","false discovery rate (fdr):\n","\n","ùêπ\n","ùê∑\n","ùëÖ\n","=\n","ùêπ\n","ùëÉ\n","/\n","(\n","ùêπ\n","ùëÉ\n","+\n","ùëá\n","ùëÉ\n",")\n","{\\displaystyle fdr=fp/(fp+tp)}\n","\n","1\n","√∑\n","(\n","1\n","+\n","11\n",")\n","=\n","8.30\n","%{\\displaystyle 1\\div (1+11)=8.30\\%}\n","\n","false omission rate (for):\n","\n","ùêπ\n","ùëÇ\n","ùëÖ\n","=\n","ùêπ\n","ùëÅ\n","/\n","(\n","ùêπ\n","ùëÅ\n","+\n","ùëá\n","ùëÅ\n",")\n","{\\displaystyle for=fn/(fn+tn)}\n","\n","45\n","√∑\n","(\n","45\n","+\n","105\n",")\n","=\n","30.00\n","%{\\displaystyle 45\\div (45+105)=30.00\\%}\n","\n","once we have calculated the key metrics we can make some initial conclusions on the performance of the decision tree model built. the accuracy that we calculated was 71.60%. the accuracy value is good to start but we would like to get our models as accurate as possible while maintaining the overall performance. the sensitivity value of 19.64% means that out of everyone who was actually positive for cancer tested positive. if we look at the specificity value of 99.06% we know that out of all the samples that were negative for cancer actually tested negative. when it comes to sensitivity and specificity it is important to have a balance between the two values, so if we can decrease our specificity to increase the sensitivity that would prove to be beneficial.[15] these are just a few examples on how to use these values and the meanings behind them to evaluate the decision tree model and improve upon the next iteration.\n"]}]},{"cell_type":"markdown","source":["# Tokenization and Lemmatization"],"metadata":{"id":"yU6dQN3ktUmV"}},{"cell_type":"code","source":["lemmatizer = nltk.stem.WordNetLemmatizer()\n","tokens = nltk.word_tokenize(combined_raw_doc)\n","punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)"],"metadata":{"id":"8qj1CEQBuPyV","executionInfo":{"status":"ok","timestamp":1722095051778,"user_tz":-330,"elapsed":788,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["def LemNormalize(text):\n","    return nltk.word_tokenize(re.sub(r'[^\\w\\s]', '', text.lower()))"],"metadata":{"id":"J0gKzAf-uSzX","executionInfo":{"status":"ok","timestamp":1722095053482,"user_tz":-330,"elapsed":3,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":["### Store Sentences for Searching"],"metadata":{"id":"4AREPHMntbDt"}},{"cell_type":"code","source":["sent_tokens = nltk.sent_tokenize(combined_raw_doc)"],"metadata":{"id":"24k115shuVnO","executionInfo":{"status":"ok","timestamp":1722095056296,"user_tz":-330,"elapsed":2,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":["# Greetings & Responses"],"metadata":{"id":"DzpdE1EMto6s"}},{"cell_type":"code","source":["greetings = [\n","    \"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\",\n","    \"howdy\", \"good day\", \"yo\", \"hi there\", \"hiya\"\n","]\n","greeting_responses = [\n","    \"Hi there!\", \"Hey!\", \"*nods*\", \"Hello!\", \"Greetings!\", \"Hi!\",\n","    \"Howdy!\", \"Good day to you!\", \"Yo!\", \"Hi there!\", \"Hello, how can I help?\"\n","]"],"metadata":{"id":"lb4MTkzXuYZk","executionInfo":{"status":"ok","timestamp":1722095058355,"user_tz":-330,"elapsed":2,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["how_are_you_phrases = [\n","    \"how are you\", \"how's it going\", \"how do you do\", \"how are things\"\n","]\n","how_are_you_responses = [\n","    \"I'm just a program, but I'm functioning as expected! How about you?\",\n","    \"I'm here to assist you! How can I help today?\",\n","    \"Doing well, thank you! What can I do for you today?\",\n","    \"All systems operational! How are you?\"\n","]"],"metadata":{"id":"_CEckYZsubHv","executionInfo":{"status":"ok","timestamp":1722095060872,"user_tz":-330,"elapsed":1,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["positive_responses = [\"good\", \"all good\", \"well\", \"fine\", \"okay\", \"great\"]"],"metadata":{"id":"7f5EtgaMpXhm","executionInfo":{"status":"ok","timestamp":1722095063451,"user_tz":-330,"elapsed":1,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":["### Context to track the chatbot's last question"],"metadata":{"id":"svGIcui6tyjM"}},{"cell_type":"code","source":["context = {\"last_question\": None}"],"metadata":{"id":"q-SBDTkHrbsN","executionInfo":{"status":"ok","timestamp":1722095068462,"user_tz":-330,"elapsed":630,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["def greet(sentence):\n","    for word in sentence.split():\n","        if word.lower() in greetings:\n","            return random.choice(greeting_responses)\n","        elif any(phrase in sentence.lower() for phrase in how_are_you_phrases):\n","            context[\"last_question\"] = \"how_are_you\"\n","            return random.choice(how_are_you_responses)"],"metadata":{"id":"RXBqJ3vUuddB","executionInfo":{"status":"ok","timestamp":1722095070615,"user_tz":-330,"elapsed":2,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":["### Simple Keyword-based Response Generation"],"metadata":{"id":"OwhKml-Jt5J1"}},{"cell_type":"code","source":["def find_answer(user_response):\n","    user_response = LemNormalize(user_response)\n","    best_match = \"\"\n","    max_overlap = 0\n","    for sentence in sent_tokens:\n","        tokenized_sentence = LemNormalize(sentence)\n","        common_words = set(user_response).intersection(set(tokenized_sentence))\n","        if len(common_words) > max_overlap:\n","            max_overlap = len(common_words)\n","            best_match = sentence\n","    return best_match if best_match else \"I am sorry, I don't have information on that.\""],"metadata":{"id":"P5cJt5eYug2Y","executionInfo":{"status":"ok","timestamp":1722095073748,"user_tz":-330,"elapsed":450,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":["### Recognizing and Responding to Date Queries"],"metadata":{"id":"krIYog5fuA1g"}},{"cell_type":"code","source":["def handle_special_queries(user_response):\n","    date_phrases = [\"what is the date\", \"what's the date\", \"today's date\", \"current date\", \"what date is it\"]\n","    day_phrases = [\"what day is it\", \"what's the day\", \"what is the day today\", \"what's the day today\", \"day\"]\n","    if any(phrase in user_response for phrase in date_phrases):\n","        return f\"Today's date is {datetime.now().strftime('%B %d, %Y')}.\"\n","    elif any(phrase in user_response for phrase in day_phrases):\n","        return f\"Today is {datetime.now().strftime('%A')}.\"\n","    return None"],"metadata":{"id":"4U2UD-DuwmPo","executionInfo":{"status":"ok","timestamp":1722095077188,"user_tz":-330,"elapsed":1,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["'''\n","context = {}\n","def set_context(user, context_data):\n","    context[user] = context_data\n","\n","def get_context(user):\n","    return context.get(user, {})\n","'''"],"metadata":{"id":"TtRiIx77ukhQ","executionInfo":{"status":"ok","timestamp":1722094375746,"user_tz":-330,"elapsed":527,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["### Fallback Options and Clarifying Options"],"metadata":{"id":"qkx2mZ1zuKEk"}},{"cell_type":"code","source":["def fallback():\n","    return \"I'm not sure I understand. Could you please rephrase?\"\n","\n","def clarify():\n","    return \"Do you mean...?\""],"metadata":{"id":"-Wt10M0uumqQ","executionInfo":{"status":"ok","timestamp":1722095081076,"user_tz":-330,"elapsed":4,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":["# Main Chatbot Loop"],"metadata":{"id":"DVemZ9lEuQ_u"}},{"cell_type":"code","source":["def chatbot():\n","    flag = True\n","    print(\"Ruby: My name is Ruby. Let's have a conversation! Also, if you want to exit any time, just type Bye!\")\n","\n","    while flag:\n","        user_response = input(\"You: \").lower()\n","        if user_response != 'bye':\n","            if user_response in ('thanks', 'thank you', 'thanks and bye'):\n","                flag = False\n","                print(\"Ruby: You're welcome! Have a nice day!\")\n","            else:\n","                if context.get(\"last_question\") == \"how_are_you\" and any(resp in user_response for resp in positive_responses):\n","                    print(\"Ruby: How can I help you?\")\n","                    context[\"last_question\"] = None\n","                else:\n","                    greeting_response = greet(user_response)\n","                    if greeting_response:\n","                        print(\"Ruby:\", greeting_response)\n","                    else:\n","                        special_response = handle_special_queries(user_response)\n","                        if special_response:\n","                            print(\"Ruby:\", special_response)\n","                        else:\n","                            response_text = find_answer(user_response)\n","                            if response_text:\n","                                print(\"Ruby:\", response_text)\n","                            else:\n","                                print(\"Ruby:\", clarify())\n","                                print(\"Ruby:\", fallback())\n","        else:\n","            flag = False\n","            print(\"Ruby: Bye! Take care...\")"],"metadata":{"id":"ZjYj9lNmrvgg","executionInfo":{"status":"ok","timestamp":1722095439491,"user_tz":-330,"elapsed":441,"user":{"displayName":"Subhadeep Dey","userId":"03492811423952281481"}}},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":["# Executing the Bot"],"metadata":{"id":"eeQcFLfTuUPD"}},{"cell_type":"code","source":["chatbot()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"htQlE1ZkusRn","outputId":"216f50de-d9ca-4e84-b560-b4c2fa83a595"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ruby: My name is Ruby. Let's have a conversation! Also, if you want to exit any time, just type Bye!\n","You: hi\n","Ruby: Yo!\n","You: how are you\n","Ruby: All systems operational! How are you?\n","You: all good\n","Ruby: How can I help you?\n","You: what is the date\n","Ruby: Today's date is July 27, 2024.\n","You: what is data science\n","Ruby: data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processes, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.\n","You: what is decision tree\n","Ruby: the left tree is the decision tree we obtain from using information gain to split the nodes and the right tree is what we obtain from using the phi function to split the nodes.\n","You: what is the day\n","Ruby: Today is Saturday.\n","You: who coined the term data science\n","Ruby: [20] in 1985, in a lecture given to the chinese academy of sciences in beijing, c. f. jeff wu used the term \"data science\" for the first time as an alternative name for statistics.\n"]}]}]}